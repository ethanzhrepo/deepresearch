# DeepResearch å·¥å…·ç³»ç»Ÿ

## ğŸ› ï¸ å·¥å…·æ¦‚è§ˆ

DeepResearch æ‹¥æœ‰å¼ºå¤§çš„å·¥å…·ç³»ç»Ÿï¼Œæ”¯æŒä»£ç æ‰§è¡Œã€ç½‘é¡µæµè§ˆã€æ–‡ä»¶æ“ä½œã€æ•°æ®åˆ†æã€æ™ºèƒ½æµè§ˆå™¨è‡ªåŠ¨åŒ–ç­‰å¤šç§åŠŸèƒ½ã€‚

## ğŸ”§ æ ¸å¿ƒå·¥å…·

### 1. ä»£ç æ‰§è¡Œå·¥å…· (CodeTool)

**åŠŸèƒ½ï¼š** å®‰å…¨æ‰§è¡Œ Python ä»£ç ï¼Œæ”¯æŒæ•°æ®åˆ†æã€å¯è§†åŒ–ç­‰ä»»åŠ¡

**é…ç½®ï¼š**
```yaml
tools:
  code_tool:
    enabled: true
    execution_environment: "docker"  # docker, local, sandbox
    timeout: 30
    max_memory_mb: 512
    allowed_packages:
      - "numpy"
      - "pandas"
      - "matplotlib"
      - "seaborn"
      - "scikit-learn"
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# æ•°æ®åˆ†æä»£ç 
import pandas as pd
import matplotlib.pyplot as plt

# åˆ›å»ºç¤ºä¾‹æ•°æ®
data = {'å¹´ä»½': [2020, 2021, 2022, 2023], 
        'ç”¨æˆ·æ•°': [100, 250, 500, 800]}
df = pd.DataFrame(data)

# ç»˜åˆ¶å›¾è¡¨
plt.figure(figsize=(10, 6))
plt.plot(df['å¹´ä»½'], df['ç”¨æˆ·æ•°'], marker='o')
plt.title('ç”¨æˆ·å¢é•¿è¶‹åŠ¿')
plt.xlabel('å¹´ä»½')
plt.ylabel('ç”¨æˆ·æ•°ï¼ˆä¸‡ï¼‰')
plt.grid(True)
plt.show()
```

### 2. Browser-Use å·¥å…· â­ **å…¨æ–°åŠŸèƒ½**

**åŠŸèƒ½ï¼š** AI é©±åŠ¨çš„æ™ºèƒ½æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼Œæ”¯æŒå¤æ‚çš„ç½‘é¡µæ“ä½œä»»åŠ¡

**é…ç½®ï¼š**
```yaml
tools:
  browser_use_tool:
    enabled: true
    llm_provider: "deepseek"  # openai, claude, gemini, deepseek
    llm_model: "deepseek-chat"
    browser:
      headless: true
      timeout: 300
      max_steps: 50
    features:
      search_and_extract: true
      form_filling: true
      custom_tasks: true
      screenshots: true
```

**æ ¸å¿ƒåŠŸèƒ½ï¼š**

#### æœç´¢å’Œæå–
```python
# æœç´¢å’Œå†…å®¹æå–
task_config = {
    "search_query": "äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•",
    "target_websites": ["arxiv.org", "scholar.google.com"],
    "extract_elements": ["title", "abstract", "authors"],
    "max_pages": 5
}

result = await browser_tool.search_and_extract(task_config)
```

#### è¡¨å•å¡«å†™
```python
# æ™ºèƒ½è¡¨å•å¡«å†™
form_config = {
    "url": "https://example.com/contact",
    "form_data": {
        "name": "ç ”ç©¶åŠ©æ‰‹",
        "email": "research@example.com",
        "message": "è‡ªåŠ¨åŒ–ç ”ç©¶æŸ¥è¯¢"
    },
    "submit": True
}

result = await browser_tool.fill_form(form_config)
```

#### è‡ªå®šä¹‰ä»»åŠ¡
```python
# å¤æ‚è‡ªå®šä¹‰ä»»åŠ¡
custom_task = {
    "task_description": "åœ¨GitHubä¸Šæœç´¢Pythonæœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œæå–é¡¹ç›®ä¿¡æ¯",
    "steps": [
        {"action": "navigate", "url": "https://github.com"},
        {"action": "search", "query": "python machine learning"},
        {"action": "extract", "selector": ".repo-list-item", "limit": 10}
    ]
}

result = await browser_tool.execute_custom_task(custom_task)
```

### 3. ä¼ ç»Ÿæµè§ˆå™¨å·¥å…· (BrowserTool)

**åŠŸèƒ½ï¼š** åŸºç¡€çš„ç½‘é¡µæµè§ˆã€æ•°æ®æŠ“å–ã€æˆªå›¾ç­‰

**é…ç½®ï¼š**
```yaml
tools:
  browser_tool:
    enabled: true
    headless: true
    timeout: 30
    max_pages: 10
    options:
      window_size: "1920x1080"
      user_agent: "DeepResearch Bot 1.0"
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```json
{
  "action": "navigate",
  "url": "https://example.com",
  "wait_for": "body"
}

{
  "action": "extract_text",
  "selector": ".content",
  "attribute": "text"
}

{
  "action": "screenshot",
  "filename": "page_screenshot.png"
}
```

### 4. æ™ºèƒ½æœç´¢å·¥å…· (SearchTool)

**åŠŸèƒ½ï¼š** å¤šæœç´¢å¼•æ“é›†æˆï¼Œæ™ºèƒ½æœç´¢ç­–ç•¥

**æ”¯æŒçš„æœç´¢å¼•æ“ï¼š**
- **Tavily Search** â­ ä¸“ä¸º AI åº”ç”¨è®¾è®¡çš„ä¸“ä¸šæœç´¢
- **DuckDuckGo** - æ³¨é‡éšç§çš„å…è´¹æœç´¢
- **ArXiv** â­ ä¸“ä¸šå­¦æœ¯è®ºæ–‡æœç´¢
- **Google Search** - é€šè¿‡ SerpAPI é›†æˆ
- **Bing Search** - å¾®è½¯å¿…åº”æœç´¢
- **Brave Search** â­ æ³¨é‡éšç§çš„ç‹¬ç«‹æœç´¢
- **Google Docs** â­ ä¸“é—¨çš„æ–‡æ¡£æœç´¢
- **Authority Sites** â­ æƒå¨ç½‘ç«™æœç´¢

**é…ç½®ï¼š**
```yaml
search:
  default_engine: tavily  # æ¨èä½¿ç”¨ Tavily
  engines:
    tavily:
      enabled: true
      include_answer: true
      include_raw_content: false
    duckduckgo:
      enabled: true
      region: cn-zh
      safe_search: moderate
    arxiv:
      enabled: true
      max_results: 10
      sort_by: "relevance"
    google:
      enabled: false  # éœ€è¦ SerpAPI
    bing:
      enabled: false  # éœ€è¦ Bing API
    brave:
      enabled: false  # éœ€è¦ Brave API
```

**æœç´¢ç­–ç•¥ï¼š**
- **æ™ºèƒ½ç­–ç•¥**: æ ¹æ®æŸ¥è¯¢ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³æœç´¢å¼•æ“
- **è½®è¯¢ç­–ç•¥**: ä¾æ¬¡ä½¿ç”¨ä¸åŒæœç´¢å¼•æ“
- **ä¼˜å…ˆçº§ç­–ç•¥**: æŒ‰é…ç½®çš„ä¼˜å…ˆçº§é€‰æ‹©
- **å¤šå¼•æ“å¯¹æ¯”**: åŒæ—¶ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“å¹¶å¯¹æ¯”ç»“æœ

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
# åŸºç¡€æœç´¢
results = search_tool.search("äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿", max_results=10)

# æŒ‡å®šæœç´¢å¼•æ“
results = search_tool.search("machine learning", engine="arxiv")

# å¤šå¼•æ“æœç´¢
multi_results = search_tool.search_multiple_engines(
    "quantum computing",
    engines=["tavily", "arxiv", "google"]
)

# å­¦æœ¯è®ºæ–‡æœç´¢
papers = search_tool.search("deep learning", engine="arxiv", max_results=20)
```

### 5. æ–‡ä»¶å·¥å…· (FileTool)

**åŠŸèƒ½ï¼š** æœ¬åœ°æ–‡ä»¶å’Œäº‘å­˜å‚¨æ“ä½œ

**é…ç½®ï¼š**
```yaml
tools:
  file_tool:
    enabled: true
    allowed_extensions:
      - ".txt"
      - ".md"
      - ".json"
      - ".csv"
      - ".pdf"
    max_file_size_mb: 10
    cloud_storage:
      google_drive: true
      dropbox: true
```

**æ”¯æŒæ“ä½œï¼š**
- è¯»å–æœ¬åœ°æ–‡ä»¶
- å†™å…¥æ–‡ä»¶
- Google Drive é›†æˆ
- Dropbox é›†æˆ
- PDF æ–‡æ¡£è§£æ

## ğŸ” ä¸“ä¸šåˆ†æå·¥å…·

### æ•°æ®åˆ†æå·¥å…·

**ç»Ÿè®¡åˆ†æï¼š**
```python
def statistical_analysis(data):
    """ç»Ÿè®¡åˆ†æåŠŸèƒ½"""
    import pandas as pd
    import numpy as np
    
    # åŸºç¡€ç»Ÿè®¡
    stats = {
        'count': len(data),
        'mean': np.mean(data),
        'median': np.median(data),
        'std': np.std(data),
        'min': np.min(data),
        'max': np.max(data)
    }
    
    return stats
```

**å†…å®¹åˆ†æï¼š**
```python
def content_analysis(text_data):
    """å†…å®¹åˆ†æåŠŸèƒ½"""
    from collections import Counter
    import re
    
    # è¯é¢‘åˆ†æ
    words = re.findall(r'\w+', text_data.lower())
    word_freq = Counter(words)
    
    # å…³é”®è¯æå–
    keywords = word_freq.most_common(10)
    
    return {
        'word_count': len(words),
        'unique_words': len(word_freq),
        'keywords': keywords
    }
```

**è¶‹åŠ¿åˆ†æï¼š**
```python
def trend_analysis(time_series_data):
    """è¶‹åŠ¿åˆ†æåŠŸèƒ½"""
    import pandas as pd
    from scipy import stats
    
    # çº¿æ€§è¶‹åŠ¿
    x = range(len(time_series_data))
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, time_series_data)
    
    return {
        'trend_slope': slope,
        'correlation': r_value,
        'p_value': p_value,
        'trend_direction': 'increasing' if slope > 0 else 'decreasing'
    }
```

### å¯è§†åŒ–å·¥å…·

**å›¾è¡¨ç”Ÿæˆï¼š**
```python
def create_visualization(data, chart_type='line'):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    plt.figure(figsize=(12, 8))
    
    if chart_type == 'line':
        plt.plot(data['x'], data['y'])
    elif chart_type == 'bar':
        plt.bar(data['x'], data['y'])
    elif chart_type == 'scatter':
        plt.scatter(data['x'], data['y'])
    elif chart_type == 'heatmap':
        sns.heatmap(data, annot=True)
    
    plt.title('æ•°æ®å¯è§†åŒ–')
    plt.xlabel('Xè½´')
    plt.ylabel('Yè½´')
    plt.grid(True)
    plt.show()
```

## ğŸ”’ å®‰å…¨æœºåˆ¶

### ä»£ç æ‰§è¡Œå®‰å…¨

**Docker æ²™ç®±ï¼ˆæ¨èï¼‰ï¼š**
```yaml
tools:
  code_tool:
    execution_environment: "docker"
    docker_config:
      image: "python:3.11-slim"
      memory_limit: "512m"
      cpu_limit: "0.5"
      network_mode: "none"
      read_only: true
```

**æœ¬åœ°æ²™ç®±ï¼š**
```yaml
tools:
  code_tool:
    execution_environment: "sandbox"
    sandbox_config:
      restricted_imports: true
      no_file_access: true
      no_network_access: true
      timeout: 30
```

### ç½‘ç»œè®¿é—®æ§åˆ¶

**æµè§ˆå™¨å®‰å…¨ï¼š**
```yaml
tools:
  browser_tool:
    security:
      allowed_domains:
        - "*.wikipedia.org"
        - "*.github.com"
        - "*.stackoverflow.com"
      blocked_domains:
        - "*.malicious-site.com"
      max_redirects: 5
```

## ğŸš€ å·¥å…·æ‰©å±•

### è‡ªå®šä¹‰å·¥å…·å¼€å‘

**åˆ›å»ºè‡ªå®šä¹‰å·¥å…·ï¼š**
```python
from tools.base_tool import BaseTool
from typing import Dict, Any

class CustomAnalysisTool(BaseTool):
    """è‡ªå®šä¹‰åˆ†æå·¥å…·"""
    
    name = "custom_analysis"
    description = "æ‰§è¡Œè‡ªå®šä¹‰æ•°æ®åˆ†æ"
    
    def _run(self, data: str, analysis_type: str) -> str:
        """æ‰§è¡Œåˆ†æ"""
        if analysis_type == "sentiment":
            return self._sentiment_analysis(data)
        elif analysis_type == "keyword":
            return self._keyword_extraction(data)
        else:
            return "ä¸æ”¯æŒçš„åˆ†æç±»å‹"
    
    def _sentiment_analysis(self, text: str) -> str:
        """æƒ…æ„Ÿåˆ†æ"""
        # å®ç°æƒ…æ„Ÿåˆ†æé€»è¾‘
        pass
    
    def _keyword_extraction(self, text: str) -> str:
        """å…³é”®è¯æå–"""
        # å®ç°å…³é”®è¯æå–é€»è¾‘
        pass
```

**æ³¨å†Œè‡ªå®šä¹‰å·¥å…·ï¼š**
```python
from tools.registry import ToolRegistry

# æ³¨å†Œå·¥å…·
registry = ToolRegistry()
registry.register_tool(CustomAnalysisTool())

# åœ¨ Agent ä¸­ä½¿ç”¨
tools = registry.get_tools(['custom_analysis'])
```

### å·¥å…·ç»„åˆ

**åˆ›å»ºå·¥å…·é“¾ï¼š**
```python
class ResearchToolChain:
    """ç ”ç©¶å·¥å…·é“¾"""
    
    def __init__(self):
        self.search_tool = SearchTool()
        self.browser_tool = BrowserTool()
        self.analysis_tool = AnalysisTool()
    
    async def research_workflow(self, topic: str):
        """å®Œæ•´ç ”ç©¶æµç¨‹"""
        # 1. æœç´¢ç›¸å…³ä¿¡æ¯
        search_results = await self.search_tool.search(topic)
        
        # 2. æµè§ˆè¯¦ç»†é¡µé¢
        detailed_info = []
        for result in search_results[:5]:
            content = await self.browser_tool.extract_content(result.url)
            detailed_info.append(content)
        
        # 3. åˆ†ææ•°æ®
        analysis = await self.analysis_tool.analyze(detailed_info)
        
        return analysis
```

## ğŸ“Š å·¥å…·ç›‘æ§

### æ€§èƒ½ç›‘æ§

**å·¥å…·ä½¿ç”¨ç»Ÿè®¡ï¼š**
```python
class ToolMonitor:
    """å·¥å…·ç›‘æ§å™¨"""
    
    def __init__(self):
        self.usage_stats = {}
        self.performance_metrics = {}
    
    def track_tool_usage(self, tool_name: str, execution_time: float):
        """è·Ÿè¸ªå·¥å…·ä½¿ç”¨"""
        if tool_name not in self.usage_stats:
            self.usage_stats[tool_name] = {
                'count': 0,
                'total_time': 0,
                'avg_time': 0
            }
        
        stats = self.usage_stats[tool_name]
        stats['count'] += 1
        stats['total_time'] += execution_time
        stats['avg_time'] = stats['total_time'] / stats['count']
    
    def get_performance_report(self):
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'usage_stats': self.usage_stats,
            'top_used_tools': sorted(
                self.usage_stats.items(),
                key=lambda x: x[1]['count'],
                reverse=True
            )[:5]
        }
```

### é”™è¯¯å¤„ç†

**å·¥å…·é”™è¯¯æ¢å¤ï¼š**
```python
class ToolErrorHandler:
    """å·¥å…·é”™è¯¯å¤„ç†å™¨"""
    
    def __init__(self):
        self.retry_strategies = {
            'network_error': self._network_retry,
            'timeout_error': self._timeout_retry,
            'resource_error': self._resource_retry
        }
    
    async def handle_tool_error(self, tool, error, context):
        """å¤„ç†å·¥å…·é”™è¯¯"""
        error_type = self._classify_error(error)
        
        if error_type in self.retry_strategies:
            return await self.retry_strategies[error_type](tool, context)
        else:
            return self._fallback_strategy(tool, context)
    
    def _classify_error(self, error):
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        if "network" in str(error).lower():
            return "network_error"
        elif "timeout" in str(error).lower():
            return "timeout_error"
        elif "memory" in str(error).lower():
            return "resource_error"
        else:
            return "unknown_error"
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å·¥å…·é€‰æ‹©ç­–ç•¥

```python
def select_optimal_tool(task_type: str, context: Dict[str, Any]):
    """é€‰æ‹©æœ€ä¼˜å·¥å…·"""
    if task_type == "data_analysis":
        if context.get('data_size', 0) > 1000000:
            return "distributed_analysis_tool"
        else:
            return "standard_analysis_tool"
    
    elif task_type == "web_scraping":
        if context.get('javascript_required', False):
            return "browser_tool"
        else:
            return "http_tool"
    
    elif task_type == "text_processing":
        if context.get('language') != 'en':
            return "multilingual_nlp_tool"
        else:
            return "standard_nlp_tool"
```

### 2. èµ„æºç®¡ç†

```python
class ToolResourceManager:
    """å·¥å…·èµ„æºç®¡ç†å™¨"""
    
    def __init__(self):
        self.resource_pools = {}
        self.usage_limits = {
            'browser_tool': 5,  # æœ€å¤š5ä¸ªæµè§ˆå™¨å®ä¾‹
            'code_tool': 3,     # æœ€å¤š3ä¸ªä»£ç æ‰§è¡Œç¯å¢ƒ
        }
    
    async def acquire_tool_resource(self, tool_name: str):
        """è·å–å·¥å…·èµ„æº"""
        if tool_name not in self.resource_pools:
            self.resource_pools[tool_name] = asyncio.Semaphore(
                self.usage_limits.get(tool_name, 10)
            )
        
        return await self.resource_pools[tool_name].acquire()
```

### 3. ç¼“å­˜ç­–ç•¥

```python
class ToolCache:
    """å·¥å…·ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = {
            'search_results': 3600,    # æœç´¢ç»“æœç¼“å­˜1å°æ—¶
            'web_content': 1800,       # ç½‘é¡µå†…å®¹ç¼“å­˜30åˆ†é’Ÿ
            'analysis_results': 7200,  # åˆ†æç»“æœç¼“å­˜2å°æ—¶
        }
    
    def get_cached_result(self, tool_name: str, input_hash: str):
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = f"{tool_name}:{input_hash}"
        
        if cache_key in self.cache:
            result, timestamp = self.cache[cache_key]
            ttl = self.cache_ttl.get(tool_name, 3600)
            
            if time.time() - timestamp < ttl:
                return result
            else:
                del self.cache[cache_key]
        
        return None
```

## ğŸ”§ é…ç½®ç¤ºä¾‹

### å®Œæ•´å·¥å…·é…ç½®

```yaml
tools:
  # ä»£ç æ‰§è¡Œå·¥å…·
  code_tool:
    enabled: true
    execution_environment: "docker"
    timeout: 60
    max_memory_mb: 1024
    allowed_packages:
      - "numpy"
      - "pandas"
      - "matplotlib"
      - "seaborn"
      - "scikit-learn"
      - "requests"
      - "beautifulsoup4"
    
  # æµè§ˆå™¨å·¥å…·
  browser_tool:
    enabled: true
    headless: true
    timeout: 30
    max_pages: 20
    options:
      window_size: "1920x1080"
      user_agent: "DeepResearch Bot 2.0"
      disable_images: false
      disable_javascript: false
    
  # æœç´¢å·¥å…·
  search_tool:
    enabled: true
    timeout: 30
    max_concurrent_searches: 5
    engines:
      google:
        priority: 1
        enabled: true
      bing:
        priority: 2
        enabled: true
      serpapi:
        priority: 3
        enabled: true
    
  # æ–‡ä»¶å·¥å…·
  file_tool:
    enabled: true
    allowed_extensions:
      - ".txt"
      - ".md"
      - ".json"
      - ".csv"
      - ".pdf"
      - ".docx"
    max_file_size_mb: 50
    
  # åˆ†æå·¥å…·
  analysis_tool:
    enabled: true
    analysis_types:
      - "statistical"
      - "content"
      - "trend"
      - "sentiment"
    max_data_points: 100000
```

## ğŸŒ BrowserUseTool - AI é©±åŠ¨çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–

### æ¦‚è¿°

BrowserUseTool é›†æˆäº† [browser-use](https://github.com/browser-use/browser-use) åº“ï¼Œæä¾› AI é©±åŠ¨çš„æ™ºèƒ½æµè§ˆå™¨è‡ªåŠ¨åŒ–åŠŸèƒ½ã€‚è¿™ä¸ªå·¥å…·å¯ä»¥è®© AI ä»£ç†åƒäººç±»ä¸€æ ·æ“ä½œæµè§ˆå™¨ï¼Œæ‰§è¡Œå¤æ‚çš„ç½‘é¡µäº¤äº’ä»»åŠ¡ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **æ™ºèƒ½ç½‘é¡µå¯¼èˆª**: AI è‡ªåŠ¨ç†è§£é¡µé¢ç»“æ„å¹¶æ‰§è¡Œå¯¼èˆª
- **è¡¨å•è‡ªåŠ¨å¡«å†™**: æ™ºèƒ½è¯†åˆ«å’Œå¡«å†™å„ç§è¡¨å•
- **æ•°æ®æå–**: ä»å¤æ‚ç½‘é¡µä¸­æå–ç»“æ„åŒ–æ•°æ®
- **å·¥ä½œæµè‡ªåŠ¨åŒ–**: æ‰§è¡Œå¤šæ­¥éª¤çš„å¤æ‚æµè§ˆå™¨æ“ä½œ
- **å®æ—¶ç›‘æ§**: ç›‘æ§ç½‘é¡µå˜åŒ–å¹¶è‡ªåŠ¨å“åº”
- **å¤šæœç´¢å¼•æ“æ”¯æŒ**: åœ¨ä¸åŒæœç´¢å¼•æ“ä¸Šæ‰§è¡Œæ™ºèƒ½æœç´¢

### é…ç½®

```yaml
tools:
  browser_use_tool:
    enabled: true
    llm_provider: openai  # å¯é€‰: openai, anthropic, google
    llm_model: gpt-4o
    browser:
      headless: true
      timeout: 300
      max_steps: 50
      save_screenshots: true
      extract_data: true
    output_dir: browser_outputs
    features:
      search_and_extract: true
      navigate_and_extract: true
      fill_form: true
      monitor_changes: true
      automate_workflow: true
      custom_task: true
    security:
      allowed_domains: []  # ç©ºæ•°ç»„è¡¨ç¤ºå…è®¸æ‰€æœ‰åŸŸå
      blocked_domains:
        - malicious-site.com
      max_execution_time: 600
      enable_content_filtering: true
```

### ä¸»è¦åŠŸèƒ½

#### 1. æ™ºèƒ½æœç´¢å’Œæå–

```python
# åœ¨å¤šä¸ªæœç´¢å¼•æ“ä¸Šæœç´¢å¹¶æå–ä¿¡æ¯
result = browser_tool.execute(
    action="search_and_extract",
    query="äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨",
    search_engine="google"
)

print(f"æœç´¢ç»“æœ: {result['extracted_data']}")
```

#### 2. ç½‘é¡µå¯¼èˆªå’Œæ•°æ®æå–

```python
# å¯¼èˆªåˆ°æŒ‡å®šç½‘é¡µå¹¶æå–ä¿¡æ¯
result = browser_tool.execute(
    action="navigate_and_extract",
    url="https://example.com/research-paper",
    extraction_task="æå–è®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦å’Œå…³é”®è¯"
)

print(f"æå–çš„æ•°æ®: {result['extracted_data']}")
```

#### 3. è¡¨å•è‡ªåŠ¨å¡«å†™

```python
# è‡ªåŠ¨å¡«å†™è¡¨å•
result = browser_tool.execute(
    action="fill_form",
    url="https://example.com/contact",
    form_data={
        "name": "ç ”ç©¶å‘˜",
        "email": "researcher@example.com",
        "message": "è¯·æä¾›æ›´å¤šå…³äºAIç ”ç©¶çš„ä¿¡æ¯"
    },
    submit=True
)

print(f"è¡¨å•æäº¤ç»“æœ: {result['result']}")
```

#### 4. é¡µé¢ç›‘æ§

```python
# ç›‘æ§é¡µé¢å˜åŒ–
result = browser_tool.execute(
    action="monitor_changes",
    url="https://news.example.com",
    element_selector="æ–°é—»æ ‡é¢˜",
    check_interval=3600,  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    max_checks=24  # æœ€å¤šæ£€æŸ¥24æ¬¡
)

print(f"ç›‘æ§è®¾ç½®: {result}")
```

#### 5. å¤æ‚å·¥ä½œæµè‡ªåŠ¨åŒ–

```python
# æ‰§è¡Œå¤æ‚çš„è‡ªåŠ¨åŒ–å·¥ä½œæµ
workflow_steps = [
    {'action': 'navigate', 'target': 'https://scholar.google.com'},
    {'action': 'type', 'target': 'æœç´¢æ¡†', 'value': 'machine learning healthcare'},
    {'action': 'click', 'target': 'æœç´¢æŒ‰é’®'},
    {'action': 'wait', 'target': 'æœç´¢ç»“æœ'},
    {'action': 'extract', 'target': 'å‰10ä¸ªæœç´¢ç»“æœçš„æ ‡é¢˜å’Œé“¾æ¥'},
    {'action': 'click', 'target': 'ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ'},
    {'action': 'extract', 'target': 'è®ºæ–‡æ‘˜è¦å’Œå¼•ç”¨ä¿¡æ¯'}
]

result = browser_tool.execute(
    action="automate_workflow",
    workflow_steps=workflow_steps
)

print(f"å·¥ä½œæµæ‰§è¡Œç»“æœ: {result}")
```

#### 6. è‡ªå®šä¹‰ä»»åŠ¡

```python
# æ‰§è¡Œè‡ªå®šä¹‰æµè§ˆå™¨ä»»åŠ¡
result = browser_tool.execute(
    action="custom_task",
    task_description="""
    è®¿é—® GitHub ä¸Šçš„ browser-use é¡¹ç›®é¡µé¢ï¼Œ
    æå–é¡¹ç›®æè¿°ã€æ˜Ÿæ ‡æ•°ã€forkæ•°ã€æœ€æ–°å‘å¸ƒç‰ˆæœ¬ä¿¡æ¯ï¼Œ
    å¹¶æŸ¥çœ‹ README æ–‡ä»¶ä¸­çš„å®‰è£…è¯´æ˜ã€‚
    """,
    url="https://github.com/browser-use/browser-use",
    max_steps=30,
    headless=True,
    timeout=300
)

print(f"ä»»åŠ¡æ‰§è¡Œç»“æœ: {result}")
```

### é«˜çº§ç”¨ä¾‹

#### ç«äº‰å¯¹æ‰‹åˆ†æ

```python
async def competitive_analysis():
    """è‡ªåŠ¨åŒ–ç«äº‰å¯¹æ‰‹åˆ†æ"""
    
    competitors = [
        'https://competitor1.com',
        'https://competitor2.com',
        'https://competitor3.com'
    ]
    
    analysis_results = []
    
    for competitor in competitors:
        # åˆ†ææ¯ä¸ªç«äº‰å¯¹æ‰‹çš„ç½‘ç«™
        workflow = [
            {'action': 'navigate', 'target': competitor},
            {'action': 'extract', 'target': 'å…¬å¸ä¿¡æ¯å’Œäº§å“ä»‹ç»'},
            {'action': 'navigate', 'target': f'{competitor}/pricing'},
            {'action': 'extract', 'target': 'ä»·æ ¼ä¿¡æ¯'},
            {'action': 'navigate', 'target': f'{competitor}/about'},
            {'action': 'extract', 'target': 'å›¢é˜Ÿå’Œå…¬å¸èƒŒæ™¯'}
        ]
        
        result = browser_tool.execute(
            action="automate_workflow",
            workflow_steps=workflow
        )
        
        analysis_results.append({
            'competitor': competitor,
            'analysis': result
        })
    
    return analysis_results
```

#### å¸‚åœºç ”ç©¶è‡ªåŠ¨åŒ–

```python
async def market_research():
    """è‡ªåŠ¨åŒ–å¸‚åœºç ”ç©¶"""
    
    research_sites = [
        'https://statista.com',
        'https://marketresearch.com',
        'https://gartner.com'
    ]
    
    market_data = []
    
    for site in research_sites:
        result = browser_tool.execute(
            action="custom_task",
            task_description=f"""
            åœ¨ {site} ä¸Šæœç´¢äººå·¥æ™ºèƒ½å¸‚åœºæ•°æ®ï¼Œæå–ï¼š
            1. å¸‚åœºè§„æ¨¡å’Œå¢é•¿é¢„æµ‹
            2. ä¸»è¦å‚ä¸è€…å’Œå¸‚åœºä»½é¢
            3. è¶‹åŠ¿å’Œé¢„æµ‹
            4. ä¸‹è½½å¯ç”¨çš„æŠ¥å‘Š
            """,
            url=site,
            max_steps=40
        )
        
        market_data.append({
            'source': site,
            'data': result
        })
    
    return market_data
```

#### ç¤¾äº¤åª’ä½“ç›‘æ§

```python
async def social_media_monitoring():
    """ç¤¾äº¤åª’ä½“ç›‘æ§"""
    
    platforms = [
        {
            'url': 'https://twitter.com',
            'search_query': '#AI #healthcare',
            'monitor_task': 'ç›‘æ§AIåŒ»ç–—ç›¸å…³çš„æ¨æ–‡å’Œè®¨è®º'
        },
        {
            'url': 'https://linkedin.com',
            'search_query': 'AI healthcare trends',
            'monitor_task': 'ç›‘æ§LinkedInä¸Šçš„AIåŒ»ç–—è¶‹åŠ¿è®¨è®º'
        }
    ]
    
    monitoring_results = []
    
    for platform in platforms:
        result = browser_tool.execute(
            action="monitor_changes",
            url=platform['url'],
            element_selector=platform['search_query'],
            check_interval=3600  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
        )
        
        monitoring_results.append({
            'platform': platform['url'],
            'monitoring_id': result.get('monitoring_id'),
            'status': result.get('status')
        })
    
    return monitoring_results
```

### å®‰å…¨è€ƒè™‘

#### åŸŸåé™åˆ¶

```yaml
security:
  allowed_domains:
    - "*.edu"
    - "*.gov"
    - "scholar.google.com"
    - "arxiv.org"
  blocked_domains:
    - "malicious-site.com"
    - "spam-site.org"
```

#### å†…å®¹è¿‡æ»¤

```yaml
security:
  enable_content_filtering: true
  content_filters:
    - block_adult_content: true
    - block_malware_sites: true
    - block_phishing_sites: true
```

#### æ‰§è¡Œé™åˆ¶

```yaml
security:
  max_execution_time: 600  # æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
  max_pages_per_session: 50  # æ¯ä¸ªä¼šè¯æœ€å¤§é¡µé¢æ•°
  rate_limiting:
    requests_per_minute: 30
    concurrent_sessions: 3
```

### é”™è¯¯å¤„ç†

```python
try:
    result = browser_tool.execute(
        action="navigate_and_extract",
        url="https://example.com",
        extraction_task="æå–é¡µé¢ä¸»è¦å†…å®¹"
    )
    
    if result['success']:
        print(f"æå–æˆåŠŸ: {result['extracted_data']}")
    else:
        print(f"æå–å¤±è´¥: {result['error']}")
        
except Exception as e:
    print(f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}")
```

### æ€§èƒ½ä¼˜åŒ–

#### ç¼“å­˜é…ç½®

```yaml
browser_use_tool:
  caching:
    enabled: true
    cache_duration: 3600  # ç¼“å­˜1å°æ—¶
    cache_size_limit: 100MB
```

#### å¹¶å‘æ§åˆ¶

```yaml
browser_use_tool:
  concurrency:
    max_concurrent_tasks: 3
    task_queue_size: 10
    timeout_per_task: 300
```

### ç›‘æ§å’Œæ—¥å¿—

#### æ‰§è¡Œæ—¥å¿—

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger('browser_use_tool').setLevel(logging.DEBUG)

# æŸ¥çœ‹æ‰§è¡Œæ­¥éª¤
result = browser_tool.execute(
    action="custom_task",
    task_description="æ‰§è¡Œå¤æ‚ä»»åŠ¡",
    verbose=True  # å¯ç”¨è¯¦ç»†è¾“å‡º
)

print(f"æ‰§è¡Œæ­¥éª¤: {result['steps_taken']}")
```

#### æ€§èƒ½ç›‘æ§

```python
# è·å–æ€§èƒ½æŒ‡æ ‡
performance = browser_tool.get_performance_metrics()
print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {performance['avg_execution_time']}")
print(f"æˆåŠŸç‡: {performance['success_rate']}")
print(f"èµ„æºä½¿ç”¨: {performance['resource_usage']}")
```

### æœ€ä½³å®è·µ

1. **ä»»åŠ¡æè¿°è¦æ¸…æ™°å…·ä½“**
   ```python
   # å¥½çš„ä»»åŠ¡æè¿°
   task = "è®¿é—®GitHubé¡¹ç›®é¡µé¢ï¼Œæå–é¡¹ç›®åç§°ã€æè¿°ã€æ˜Ÿæ ‡æ•°å’Œæœ€æ–°ç‰ˆæœ¬å·"
   
   # é¿å…æ¨¡ç³Šçš„æè¿°
   task = "è·å–ä¸€äº›é¡¹ç›®ä¿¡æ¯"
   ```

2. **åˆç†è®¾ç½®è¶…æ—¶å’Œæ­¥éª¤é™åˆ¶**
   ```python
   # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´å‚æ•°
   simple_task = {'max_steps': 10, 'timeout': 60}
   complex_task = {'max_steps': 50, 'timeout': 300}
   ```

3. **ä½¿ç”¨é”™è¯¯é‡è¯•æœºåˆ¶**
   ```python
   def execute_with_retry(action, max_retries=3, **kwargs):
       for attempt in range(max_retries):
           result = browser_tool.execute(action=action, **kwargs)
           if result['success']:
               return result
           time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
       return result
   ```

4. **å®šæœŸæ¸…ç†èµ„æº**
   ```python
   # å®šæœŸæ¸…ç†æµè§ˆå™¨ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶
   browser_tool.cleanup_resources()
   ```

### æ•…éšœæ’é™¤

#### å¸¸è§é—®é¢˜

1. **æµè§ˆå™¨å¯åŠ¨å¤±è´¥**
   ```bash
   # å®‰è£… Playwright æµè§ˆå™¨
   playwright install chromium --with-deps
   ```

2. **é¡µé¢åŠ è½½è¶…æ—¶**
   ```python
   # å¢åŠ è¶…æ—¶æ—¶é—´
   result = browser_tool.execute(
       action="navigate_and_extract",
       url="slow-website.com",
       timeout=600  # å¢åŠ åˆ°10åˆ†é’Ÿ
   )
   ```

3. **å…ƒç´ å®šä½å¤±è´¥**
   ```python
   # ä½¿ç”¨æ›´å…·ä½“çš„ä»»åŠ¡æè¿°
   task = "ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½åï¼ŒæŸ¥æ‰¾åŒ…å«'æäº¤'æ–‡å­—çš„æŒ‰é’®å¹¶ç‚¹å‡»"
   ```

### é›†æˆç¤ºä¾‹

å®Œæ•´çš„ç ”ç©¶æµç¨‹é›†æˆç¤ºä¾‹è¯·å‚è€ƒ `examples/browser_use_integration.py` æ–‡ä»¶ã€‚

---

**BrowserUseTool è®© AI ä»£ç†å…·å¤‡äº†äººç±»çº§åˆ«çš„æµè§ˆå™¨æ“ä½œèƒ½åŠ›ï¼Œå¤§å¤§æ‰©å±•äº†è‡ªåŠ¨åŒ–ç ”ç©¶çš„å¯èƒ½æ€§ï¼** ğŸŒğŸ¤–

**å¼ºå¤§çš„å·¥å…·ç³»ç»Ÿè®© DeepResearch èƒ½å¤Ÿå¤„ç†å„ç§å¤æ‚çš„ç ”ç©¶ä»»åŠ¡ï¼** ğŸ› ï¸âœ¨ 