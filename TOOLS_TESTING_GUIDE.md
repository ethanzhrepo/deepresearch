# ðŸ”¬ DeepResearch å·¥å…·æµ‹è¯•æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©æ‚¨éªŒè¯ DeepResearch ç³»ç»Ÿçš„æ‰€æœ‰å·¥å…·å’ŒåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

## ðŸš€ å¿«é€ŸéªŒè¯

ä¸€é”®æµ‹è¯•æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š

```bash
# è¿è¡Œå®Œæ•´çš„åŠŸèƒ½æµ‹è¯•
python test_fixes.py

# æ£€æŸ¥é…ç½®çŠ¶æ€
./run.sh config-check

# è¿è¡Œæ¼”ç¤º
./run.sh demo
```

## ðŸ” è¯¦ç»†å·¥å…·æµ‹è¯•

### 1. æœç´¢å¼•æ“Žæµ‹è¯•

#### æµ‹è¯•æ‰€æœ‰æœç´¢å¼•æ“Ž
```python
# åˆ›å»ºæœç´¢å¼•æ“Žæµ‹è¯•è„šæœ¬
cat > test_search_engines.py << 'EOF'
#!/usr/bin/env python3
"""æµ‹è¯•æ‰€æœ‰æœç´¢å¼•æ“ŽåŠŸèƒ½"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tools.search_engines import SearchEngineManager

def test_all_search_engines():
    """æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æœç´¢å¼•æ“Ž"""
    print("ðŸ” æµ‹è¯•æœç´¢å¼•æ“Ž...")
    
    manager = SearchEngineManager()
    
    # æ˜¾ç¤ºå¯ç”¨çš„æœç´¢å¼•æ“Ž
    available_engines = manager.get_available_engines()
    print(f"å¯ç”¨æœç´¢å¼•æ“Ž: {available_engines}")
    
    test_query = "DeepSeek LLM"
    
    for engine in available_engines:
        print(f"\nðŸ” æµ‹è¯• {engine} æœç´¢å¼•æ“Ž:")
        try:
            results = manager.search(test_query, engine=engine, max_results=3)
            if results:
                print(f"  âœ… æˆåŠŸèŽ·å– {len(results)} ä¸ªç»“æžœ")
                for i, result in enumerate(results[:2], 1):
                    print(f"  {i}. {result.title[:60]}... æ¥æº: {result.source}")
                    print(f"     URL: {result.url}")
            else:
                print(f"  âš ï¸ æ²¡æœ‰èŽ·å–åˆ°ç»“æžœ")
        except Exception as e:
            print(f"  âŒ æœç´¢å¤±è´¥: {e}")
    
    # æµ‹è¯•å¼•ç”¨æ¥æºä¿®å¤
    print(f"\nðŸŽ¯ éªŒè¯å¼•ç”¨æ¥æºä¿®å¤:")
    print("ç¡®ä¿æ˜¾ç¤ºçš„æ˜¯åŸŸåè€Œä¸æ˜¯æœç´¢å¼•æ“Žåç§°")

if __name__ == "__main__":
    test_all_search_engines()
EOF

# è¿è¡Œæœç´¢å¼•æ“Žæµ‹è¯•
python test_search_engines.py
```

#### æµ‹è¯•ç‰¹å®šæœç´¢å¼•æ“Ž

**Tavily æœç´¢æµ‹è¯•**:
```python
from tools.search_engines import TavilySearch

# æµ‹è¯• Tavily æœç´¢
tavily = TavilySearch()
results = tavily.search("äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿", max_results=5)
print(f"Tavily æœç´¢ç»“æžœ: {len(results)} ä¸ª")
for result in results[:2]:
    print(f"æ ‡é¢˜: {result.title}")
    print(f"æ¥æº: {result.source}")  # åº”è¯¥æ˜¾ç¤ºåŸŸåï¼Œä¸æ˜¯ "tavily"
```

**ArXiv æœç´¢æµ‹è¯•**:
```python
from tools.search_engines import ArxivSearch

# æµ‹è¯• ArXiv æœç´¢
arxiv = ArxivSearch()
results = arxiv.search("machine learning", max_results=3)
print(f"ArXiv æœç´¢ç»“æžœ: {len(results)} ä¸ª")
for result in results:
    print(f"è®ºæ–‡: {result.title}")
    print(f"æ¥æº: {result.source}")  # åº”è¯¥æ˜¾ç¤º "arxiv.org"
```

**DuckDuckGo æœç´¢æµ‹è¯•**:
```python
from tools.search_engines import DuckDuckGoSearch

# æµ‹è¯• DuckDuckGo æœç´¢
ddg = DuckDuckGoSearch()
results = ddg.search("é‡å­è®¡ç®—", max_results=5)
print(f"DuckDuckGo æœç´¢ç»“æžœ: {len(results)} ä¸ª")
for result in results:
    print(f"æ ‡é¢˜: {result.title}")
    print(f"æ¥æº: {result.source}")  # åº”è¯¥æ˜¾ç¤ºå®žé™…åŸŸå
```

### 2. Browser-Use å·¥å…·æµ‹è¯•

#### åŸºç¡€åŠŸèƒ½æµ‹è¯•
```python
# åˆ›å»º Browser-Use æµ‹è¯•è„šæœ¬
cat > test_browser_use.py << 'EOF'
#!/usr/bin/env python3
"""æµ‹è¯• Browser-Use å·¥å…·åŠŸèƒ½"""

import asyncio
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tools.browser_use_tool import BrowserUseTool

async def test_browser_use():
    """æµ‹è¯• Browser-Use å·¥å…·"""
    print("ðŸŒ æµ‹è¯• Browser-Use å·¥å…·...")
    
    try:
        # åˆå§‹åŒ–å·¥å…·
        browser_tool = BrowserUseTool()
        print("âœ… Browser-Use å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æœç´¢å’Œæå–åŠŸèƒ½
        print("\nðŸ” æµ‹è¯•æœç´¢å’Œæå–åŠŸèƒ½:")
        result = browser_tool.execute(
            action="search_and_extract",
            query="äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•",
            search_engine="google",
            timeout=30
        )
        
        if result.get('success'):
            print("âœ… æœç´¢å’Œæå–åŠŸèƒ½æ­£å¸¸")
            extracted_data = result.get('extracted_data', {})
            print(f"æå–çš„æ•°æ®é¡¹: {len(extracted_data)}")
        else:
            print(f"âŒ æœç´¢å’Œæå–åŠŸèƒ½å¤±è´¥: {result.get('error')}")
        
        # æµ‹è¯•ç½‘é¡µå¯¼èˆªå’Œæå–
        print("\nðŸŒ æµ‹è¯•ç½‘é¡µå¯¼èˆªå’Œæå–:")
        result = browser_tool.execute(
            action="navigate_and_extract",
            url="https://example.com",
            extraction_task="æå–é¡µé¢æ ‡é¢˜å’Œä¸»è¦å†…å®¹",
            timeout=20
        )
        
        if result.get('success'):
            print("âœ… ç½‘é¡µå¯¼èˆªå’Œæå–åŠŸèƒ½æ­£å¸¸")
        else:
            print(f"âŒ ç½‘é¡µå¯¼èˆªå¤±è´¥: {result.get('error')}")
            
    except Exception as e:
        print(f"âŒ Browser-Use å·¥å…·æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(test_browser_use())
EOF

# è¿è¡Œ Browser-Use æµ‹è¯•
python test_browser_use.py
```

#### é«˜çº§åŠŸèƒ½æµ‹è¯•
```python
# æµ‹è¯•è¡¨å•å¡«å†™åŠŸèƒ½
async def test_form_filling():
    browser_tool = BrowserUseTool()
    
    result = browser_tool.execute(
        action="fill_form",
        url="https://httpbin.org/forms/post",
        form_data={
            "name": "Test User",
            "email": "test@example.com",
            "message": "This is a test message"
        },
        submit=False  # ä¸å®žé™…æäº¤
    )
    
    print(f"è¡¨å•å¡«å†™ç»“æžœ: {result}")

# æµ‹è¯•è‡ªå®šä¹‰ä»»åŠ¡
async def test_custom_task():
    browser_tool = BrowserUseTool()
    
    result = browser_tool.execute(
        action="custom_task",
        task_description="è®¿é—®GitHubä¸»é¡µï¼Œæå–ä¸»è¦å¯¼èˆªèœå•çš„é“¾æŽ¥",
        url="https://github.com",
        max_steps=10
    )
    
    print(f"è‡ªå®šä¹‰ä»»åŠ¡ç»“æžœ: {result}")
```

### 3. LLM é›†æˆæµ‹è¯•

#### æµ‹è¯•æ‰€æœ‰ LLM æä¾›å•†
```python
# åˆ›å»º LLM æµ‹è¯•è„šæœ¬
cat > test_llms.py << 'EOF'
#!/usr/bin/env python3
"""æµ‹è¯•æ‰€æœ‰ LLM æä¾›å•†"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import config
from llm.openai import OpenAIWrapper
from llm.claude import ClaudeWrapper
from llm.gemini import GeminiWrapper
from llm.deepseek import DeepSeekWrapper
from llm.ollama import OllamaWrapper

def test_llm_provider(provider_name, wrapper_class):
    """æµ‹è¯•å•ä¸ª LLM æä¾›å•†"""
    print(f"\nðŸ¤– æµ‹è¯• {provider_name}:")
    
    try:
        llm_config = config.get_llm_config(provider_name)
        if not llm_config.get('api_key') and provider_name != 'ollama':
            print(f"  âš ï¸ æ²¡æœ‰é…ç½® API å¯†é’¥ï¼Œè·³è¿‡æµ‹è¯•")
            return False
        
        llm = wrapper_class(llm_config)
        
        # æµ‹è¯•ç®€å•ç”Ÿæˆ
        response = llm.generate(
            prompt="è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½",
            max_tokens=100
        )
        
        if response.is_success:
            print(f"  âœ… ç”ŸæˆæˆåŠŸ: {response.content[:50]}...")
            return True
        else:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {response.error}")
            return False
            
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_all_llms():
    """æµ‹è¯•æ‰€æœ‰ LLM æä¾›å•†"""
    print("ðŸ¤– æµ‹è¯• LLM æä¾›å•†...")
    
    providers = [
        ("openai", OpenAIWrapper),
        ("claude", ClaudeWrapper),
        ("gemini", GeminiWrapper),
        ("deepseek", DeepSeekWrapper),
        ("ollama", OllamaWrapper),
    ]
    
    results = {}
    for provider_name, wrapper_class in providers:
        results[provider_name] = test_llm_provider(provider_name, wrapper_class)
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æžœ
    print(f"\nðŸ“Š LLM æµ‹è¯•ç»“æžœ:")
    for provider, success in results.items():
        status = "âœ… æ­£å¸¸" if success else "âŒ å¤±è´¥"
        print(f"  {provider}: {status}")
    
    working_providers = [p for p, s in results.items() if s]
    print(f"\nðŸŽ¯ å¯ç”¨çš„ LLM æä¾›å•†: {working_providers}")

if __name__ == "__main__":
    test_all_llms()
EOF

# è¿è¡Œ LLM æµ‹è¯•
python test_llms.py
```

#### æµ‹è¯• DeepSeek é›†æˆ
```python
# ä¸“é—¨æµ‹è¯• DeepSeek LLM
from llm.deepseek import DeepSeekWrapper
from config import config

def test_deepseek():
    llm_config = config.get_llm_config("deepseek")
    deepseek = DeepSeekWrapper(llm_config)
    
    response = deepseek.generate(
        prompt="è¯·ä»‹ç» DeepSeek æ¨¡åž‹çš„ç‰¹ç‚¹",
        max_tokens=200,
        temperature=0.7
    )
    
    print(f"DeepSeek å“åº”: {response.content}")
    print(f"Token ä½¿ç”¨: {response.token_usage}")
```

### 4. å…¶ä»–å·¥å…·æµ‹è¯•

#### ä»£ç æ‰§è¡Œå·¥å…·æµ‹è¯•
```python
from tools.code_runner import CodeTool

def test_code_execution():
    """æµ‹è¯•ä»£ç æ‰§è¡Œå·¥å…·"""
    print("ðŸ’» æµ‹è¯•ä»£ç æ‰§è¡Œå·¥å…·...")
    
    code_tool = CodeTool()
    
    # æµ‹è¯• Python ä»£ç æ‰§è¡Œ
    test_code = """
import pandas as pd
import numpy as np

# åˆ›å»ºæµ‹è¯•æ•°æ®
data = {'A': [1, 2, 3], 'B': [4, 5, 6]}
df = pd.DataFrame(data)
print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
print(df.head())
    """
    
    result = code_tool.execute(test_code)
    print(f"ä»£ç æ‰§è¡Œç»“æžœ: {result}")
```

#### æ–‡ä»¶å¤„ç†å·¥å…·æµ‹è¯•
```python
from tools.file_reader import FileTool

def test_file_operations():
    """æµ‹è¯•æ–‡ä»¶æ“ä½œå·¥å…·"""
    print("ðŸ“ æµ‹è¯•æ–‡ä»¶æ“ä½œå·¥å…·...")
    
    file_tool = FileTool()
    
    # æµ‹è¯•æ–‡ä»¶è¯»å–
    test_content = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶å†…å®¹\nåŒ…å«å¤šè¡Œæ–‡æœ¬\nç”¨äºŽæµ‹è¯•æ–‡ä»¶è¯»å–åŠŸèƒ½"
    
    # å†™å…¥æµ‹è¯•æ–‡ä»¶
    file_tool.write_file("test_file.txt", test_content)
    
    # è¯»å–æµ‹è¯•æ–‡ä»¶
    content = file_tool.read_file("test_file.txt")
    print(f"æ–‡ä»¶å†…å®¹: {content}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import os
    if os.path.exists("test_file.txt"):
        os.remove("test_file.txt")
        print("âœ… æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")
```

## ðŸ§ª å®Œæ•´ç ”ç©¶æµç¨‹æµ‹è¯•

### ç«¯åˆ°ç«¯æµ‹è¯•
```python
# åˆ›å»ºå®Œæ•´æµç¨‹æµ‹è¯•è„šæœ¬
cat > test_full_workflow.py << 'EOF'
#!/usr/bin/env python3
"""æµ‹è¯•å®Œæ•´ç ”ç©¶å·¥ä½œæµ"""

import asyncio
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from workflow.graph import ResearchWorkflow

async def test_research_workflow():
    """æµ‹è¯•å®Œæ•´ç ”ç©¶å·¥ä½œæµ"""
    print("ðŸ”¬ æµ‹è¯•å®Œæ•´ç ”ç©¶å·¥ä½œæµ...")
    
    # åˆ›å»ºç ”ç©¶å·¥ä½œæµ
    workflow = ResearchWorkflow(
        llm_provider="deepseek",  # ä½¿ç”¨ DeepSeek
        max_sections=3,          # é™åˆ¶ç« èŠ‚æ•°é‡
        interactive_mode=False   # éžäº¤äº’æ¨¡å¼
    )
    
    test_topic = "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨"
    
    print(f"ðŸ“ ç ”ç©¶ä¸»é¢˜: {test_topic}")
    
    try:
        # è¿è¡Œå®Œæ•´å·¥ä½œæµ
        outline, content_map = await workflow.run_full_workflow(test_topic)
        
        if outline and content_map:
            print("âœ… ç ”ç©¶å·¥ä½œæµå®Œæˆ")
            print(f"ðŸ“‹ ç”Ÿæˆå¤§çº²: {outline.title}")
            print(f"ðŸ“Š ç« èŠ‚æ•°é‡: {len(outline.sections)}")
            print(f"ðŸ“ å†…å®¹éƒ¨åˆ†: {len(content_map)}")
            
            # æ£€æŸ¥å¼•ç”¨æ¥æº
            total_sources = 0
            for section_key, content in content_map.items():
                sources = content.sources
                total_sources += len(sources)
                print(f"  {section_key}: {len(sources)} ä¸ªå¼•ç”¨æ¥æº")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªå¼•ç”¨æ¥æº
                for source in sources[:2]:
                    print(f"    - {source}")
            
            print(f"ðŸ“š æ€»å¼•ç”¨æ¥æº: {total_sources} ä¸ª")
            
            # éªŒè¯å¼•ç”¨æ¥æºæ ¼å¼
            print(f"\nðŸŽ¯ éªŒè¯å¼•ç”¨æ¥æºæ ¼å¼:")
            if total_sources > 0:
                print("âœ… å¼•ç”¨æ¥æºå·²åŒ…å«åœ¨å†…å®¹ä¸­")
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¼•ç”¨æ¥æº")
                
            return True
        else:
            print("âŒ ç ”ç©¶å·¥ä½œæµå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ ç ”ç©¶å·¥ä½œæµå¼‚å¸¸: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(test_research_workflow())
    if success:
        print("\nðŸŽ‰ å®Œæ•´ç ”ç©¶æµç¨‹æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâš ï¸ ç ”ç©¶æµç¨‹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
EOF

# è¿è¡Œå®Œæ•´æµç¨‹æµ‹è¯•
python test_full_workflow.py
```

### äº¤äº’å¼æµ‹è¯•
```bash
# è¿è¡Œäº¤äº’å¼ç ”ç©¶æµ‹è¯•
./run.sh interactive "æœºå™¨å­¦ä¹ ç®—æ³•æ¯”è¾ƒ" --provider deepseek

# æµ‹è¯•è‡ªåŠ¨åŒ–æ¨¡å¼
./run.sh auto "åŒºå—é“¾æŠ€æœ¯å‘å±•" --provider claude --max-sections 4
```

## ðŸŽ¨ LangGraph Studio æµ‹è¯•

### å®‰è£…å’Œé…ç½®æµ‹è¯•
```bash
# 1. éªŒè¯ LangGraph Studio é…ç½®æ–‡ä»¶
cat langgraph.json

# 2. æ£€æŸ¥ä¾èµ–é¡¹
pip list | grep -E "(langgraph|langchain)"

# 3. éªŒè¯çŽ¯å¢ƒå˜é‡
echo $LANGCHAIN_TRACING_V2
echo $LANGCHAIN_API_KEY
```

### Studio å·¥ä½œæµæµ‹è¯•
```python
# è¿è¡Œ Studio å¿«é€Ÿå¼€å§‹æ¼”ç¤º
python examples/studio_quickstart.py

# æµ‹è¯• Studio é›†æˆ
cat > test_studio_integration.py << 'EOF'
#!/usr/bin/env python3
"""æµ‹è¯• LangGraph Studio é›†æˆ"""

import asyncio
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from examples.studio_quickstart import StudioQuickstartWorkflow

async def test_studio_workflow():
    """æµ‹è¯• Studio å·¥ä½œæµ"""
    print("ðŸŽ¨ æµ‹è¯• LangGraph Studio å·¥ä½œæµ...")
    
    try:
        # åˆ›å»ºæ¼”ç¤ºå·¥ä½œæµ
        workflow = StudioQuickstartWorkflow()
        
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "topic": "LangGraph Studio æµ‹è¯•",
            "stage": "init",
            "findings": "",
            "debug_info": {}
        }
        
        # é…ç½®
        config = {"configurable": {"thread_id": "studio-test-001"}}
        
        print("âš¡ æ‰§è¡Œ Studio æ¼”ç¤ºå·¥ä½œæµ...")
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = await workflow.graph.ainvoke(initial_state, config=config)
        
        if result and result.get("stage") == "complete":
            print("âœ… Studio å·¥ä½œæµæµ‹è¯•æˆåŠŸ")
            print(f"ðŸ“Š æœ€ç»ˆçŠ¶æ€: {result['stage']}")
            print(f"ðŸ” è°ƒè¯•ä¿¡æ¯: {result.get('debug_info', {})}")
            return True
        else:
            print("âŒ Studio å·¥ä½œæµæµ‹è¯•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ Studio å·¥ä½œæµæµ‹è¯•å¼‚å¸¸: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(test_studio_workflow())
    if success:
        print("\nðŸŽ‰ LangGraph Studio é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        print("ðŸ’¡ çŽ°åœ¨å¯ä»¥åœ¨ LangGraph Studio ä¸­æ‰“å¼€é¡¹ç›®ç›®å½•è¿›è¡Œå¯è§†åŒ–è°ƒè¯•")
    else:
        print("\nâš ï¸ Studio é›†æˆæµ‹è¯•å¤±è´¥")
EOF

# è¿è¡Œ Studio é›†æˆæµ‹è¯•
python test_studio_integration.py
```

### Studio å¯è§†åŒ–éªŒè¯
```bash
# åˆ›å»º Studio éªŒè¯æŒ‡å—
cat > verify_studio.md << 'EOF'
# LangGraph Studio å¯è§†åŒ–éªŒè¯

## æ­¥éª¤ 1: æ‰“å¼€ LangGraph Studio
1. å¯åŠ¨ LangGraph Studio åº”ç”¨
2. ç™»å½• LangSmith è´¦æˆ·
3. é€‰æ‹© "Open Directory"
4. é€‰æ‹© DeepResearch é¡¹ç›®ç›®å½•

## æ­¥éª¤ 2: éªŒè¯å·¥ä½œæµå¯è§†åŒ–
- [ ] å›¾å½¢ç•Œé¢æ˜¾ç¤ºå·¥ä½œæµèŠ‚ç‚¹
- [ ] èŠ‚ç‚¹ä¹‹é—´çš„è¿žæŽ¥æ­£ç¡®æ˜¾ç¤º
- [ ] å¯ä»¥çœ‹åˆ°ä»¥ä¸‹èŠ‚ç‚¹:
  - [ ] initialize
  - [ ] search_topic  
  - [ ] analyze_results
  - [ ] generate_summary

## æ­¥éª¤ 3: æµ‹è¯•äº¤äº’åŠŸèƒ½
- [ ] åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æµ‹è¯•ä¸»é¢˜
- [ ] ç‚¹å‡»è¿è¡ŒæŒ‰é’®å¯åŠ¨å·¥ä½œæµ
- [ ] è§‚å¯ŸèŠ‚ç‚¹æ‰§è¡ŒçŠ¶æ€å˜åŒ–
- [ ] æŸ¥çœ‹å®žæ—¶çŠ¶æ€æ›´æ–°

## æ­¥éª¤ 4: è°ƒè¯•åŠŸèƒ½éªŒè¯
- [ ] è®¾ç½®æ–­ç‚¹å¹¶æš‚åœæ‰§è¡Œ
- [ ] æ£€æŸ¥çŠ¶æ€é¢æ¿ä¸­çš„æ•°æ®
- [ ] æ‰‹åŠ¨ä¿®æ”¹çŠ¶æ€å€¼
- [ ] ç»§ç»­æ‰§è¡ŒéªŒè¯ä¿®æ”¹æ•ˆæžœ

## æ­¥éª¤ 5: ç›‘æŽ§é¢æ¿éªŒè¯
- [ ] æŸ¥çœ‹æ‰§è¡Œæ—¥å¿—
- [ ] æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
- [ ] éªŒè¯é”™è¯¯å¤„ç†
EOF

echo "ðŸ“– Studio éªŒè¯æŒ‡å—å·²åˆ›å»º: verify_studio.md"
```

## ðŸš¨ æ•…éšœæŽ’é™¤å’Œè¯Šæ–­

### è‡ªåŠ¨è¯Šæ–­è„šæœ¬
```python
# åˆ›å»ºç»¼åˆè¯Šæ–­è„šæœ¬
cat > diagnose_tools.py << 'EOF'
#!/usr/bin/env python3
"""DeepResearch å·¥å…·è¯Šæ–­è„šæœ¬"""

import sys
import os
import importlib
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def diagnose_environment():
    """è¯Šæ–­çŽ¯å¢ƒé…ç½®"""
    print("ðŸ”§ è¯Šæ–­çŽ¯å¢ƒé…ç½®...")
    
    try:
        # æ£€æŸ¥ Python ç‰ˆæœ¬
        python_version = sys.version_info
        print(f"Python ç‰ˆæœ¬: {python_version.major}.{python_version.minor}")
        
        if python_version.major >= 3 and python_version.minor >= 11:
            print("âœ… Python ç‰ˆæœ¬æ»¡è¶³è¦æ±‚")
        else:
            print("âš ï¸ Python ç‰ˆæœ¬å»ºè®® 3.11+")
        
        # æ£€æŸ¥å…³é”®ä¾èµ–åŒ…
        required_packages = [
            'langchain', 'langgraph', 'openai', 'anthropic', 
            'google-generativeai', 'tavily-python', 'duckduckgo-search',
            'browser-use', 'playwright'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                importlib.import_module(package.replace('-', '_'))
                print(f"âœ… {package} å·²å®‰è£…")
            except ImportError:
                missing_packages.append(package)
                print(f"âŒ {package} æœªå®‰è£…")
        
        if not missing_packages:
            print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
            return True
        else:
            print(f"âš ï¸ ç¼ºå°‘ä¾èµ–åŒ…: {missing_packages}")
            return False
            
    except Exception as e:
        print(f"âŒ çŽ¯å¢ƒè¯Šæ–­å¤±è´¥: {e}")
        return False

def diagnose_api_keys():
    """è¯Šæ–­ API å¯†é’¥é…ç½®"""
    print("\nðŸ”‘ è¯Šæ–­ API å¯†é’¥é…ç½®...")
    
    try:
        from config import config
        
        api_keys = {
            'OPENAI_API_KEY': config.llm.openai.api_key,
            'ANTHROPIC_API_KEY': config.llm.claude.api_key,
            'GOOGLE_API_KEY': config.llm.gemini.api_key,
            'DEEPSEEK_API_KEY': config.llm.deepseek.api_key,
            'TAVILY_API_KEY': config.search.tavily.api_key,
        }
        
        configured_keys = 0
        for key_name, key_value in api_keys.items():
            if key_value and key_value.strip():
                print(f"âœ… {key_name} å·²é…ç½®")
                configured_keys += 1
            else:
                print(f"âš ï¸ {key_name} æœªé…ç½®")
        
        if configured_keys >= 2:  # è‡³å°‘éœ€è¦ä¸€ä¸ª LLM å’Œä¸€ä¸ªæœç´¢ API
            print(f"âœ… API å¯†é’¥é…ç½®å……è¶³ ({configured_keys} ä¸ª)")
            return True
        else:
            print(f"âš ï¸ å»ºè®®é…ç½®æ›´å¤š API å¯†é’¥ (å½“å‰: {configured_keys} ä¸ª)")
            return False
            
    except Exception as e:
        print(f"âŒ API å¯†é’¥è¯Šæ–­å¤±è´¥: {e}")
        return False

def diagnose_search_engines():
    """è¯Šæ–­æœç´¢å¼•æ“Ž"""
    print("\nðŸ” è¯Šæ–­æœç´¢å¼•æ“Ž...")
    
    try:
        from tools.search_engines import SearchEngineManager
        
        manager = SearchEngineManager()
        available_engines = manager.get_available_engines()
        
        print(f"å¯ç”¨æœç´¢å¼•æ“Ž: {available_engines}")
        
        if available_engines:
            # æµ‹è¯•ç¬¬ä¸€ä¸ªå¯ç”¨çš„æœç´¢å¼•æ“Ž
            test_engine = available_engines[0]
            print(f"æµ‹è¯• {test_engine} æœç´¢...")
            results = manager.search("test", engine=test_engine, max_results=1)
            if results:
                print(f"âœ… {test_engine} æœç´¢æ­£å¸¸")
                return True
            else:
                print(f"âŒ {test_engine} æœç´¢æ— ç»“æžœ")
                return False
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æœç´¢å¼•æ“Ž")
            return False
            
    except Exception as e:
        print(f"âŒ æœç´¢å¼•æ“Žè¯Šæ–­å¤±è´¥: {e}")
        return False

def diagnose_browser_use():
    """è¯Šæ–­ Browser-Use å·¥å…·"""
    print("\nðŸŒ è¯Šæ–­ Browser-Use å·¥å…·...")
    
    try:
        from tools.browser_use_tool import BrowserUseTool
        browser_tool = BrowserUseTool()
        print("âœ… Browser-Use å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ Browser-Use å·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def diagnose_studio_integration():
    """è¯Šæ–­ LangGraph Studio é›†æˆ"""
    print("\nðŸŽ¨ è¯Šæ–­ LangGraph Studio é›†æˆ...")
    
    try:
        # æ£€æŸ¥ langgraph.json é…ç½®æ–‡ä»¶
        if os.path.exists("langgraph.json"):
            print("âœ… langgraph.json é…ç½®æ–‡ä»¶å­˜åœ¨")
        else:
            print("âŒ langgraph.json é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        # æ£€æŸ¥ LangSmith çŽ¯å¢ƒå˜é‡
        langchain_api_key = os.getenv("LANGCHAIN_API_KEY")
        if langchain_api_key:
            print("âœ… LANGCHAIN_API_KEY å·²é…ç½®")
        else:
            print("âš ï¸ LANGCHAIN_API_KEY æœªé…ç½®")
        
        # æ£€æŸ¥ Studio ç¤ºä¾‹æ–‡ä»¶
        if os.path.exists("examples/studio_quickstart.py"):
            print("âœ… Studio å¿«é€Ÿå¼€å§‹ç¤ºä¾‹å­˜åœ¨")
        else:
            print("âŒ Studio å¿«é€Ÿå¼€å§‹ç¤ºä¾‹ä¸å­˜åœ¨")
            return False
        
        print("âœ… LangGraph Studio é›†æˆé…ç½®æ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ Studio é›†æˆè¯Šæ–­å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ðŸ¥ DeepResearch å·¥å…·è¯Šæ–­")
    print("=" * 50)
    
    checks = [
        diagnose_environment(),
        diagnose_api_keys(),
        diagnose_search_engines(),
        diagnose_browser_use(),
        diagnose_studio_integration()
    ]
    
    passed = sum(checks)
    total = len(checks)
    
    print(f"\nðŸ“Š è¯Šæ–­ç»“æžœ: {passed}/{total} é¡¹æ£€æŸ¥é€šè¿‡")
    
    if passed == total:
        print("ðŸŽ‰ æ‰€æœ‰å·¥å…·æ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ï¼")
        print("ðŸ’¡ çŽ°åœ¨å¯ä»¥åœ¨ LangGraph Studio ä¸­å¯è§†åŒ–è°ƒè¯•å·¥ä½œæµ")
    else:
        print("âš ï¸ éƒ¨åˆ†å·¥å…·å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
EOF

# è¿è¡Œè¯Šæ–­
python diagnose_tools.py
```

### è°ƒè¯•æ¨¡å¼æµ‹è¯•
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—è¿›è¡Œè°ƒè¯•
export DEBUG=1
python main.py research "æµ‹è¯•ä¸»é¢˜" --provider deepseek --debug
```

### ä¿®å¤å¼•ç”¨æ¥æºé—®é¢˜æµ‹è¯•
```bash
# è¿è¡Œå¼•ç”¨æ¥æºä¿®å¤æµ‹è¯•
python test_sources_fix.py
```

---

## ðŸ“ æµ‹è¯•æ£€æŸ¥æ¸…å•

ä½¿ç”¨æ­¤æ£€æŸ¥æ¸…å•ç¡®ä¿æ‰€æœ‰åŠŸèƒ½æ­£å¸¸ï¼š

- [ ] **çŽ¯å¢ƒé…ç½®**: Python ä¾èµ–åŒ…å·²å®‰è£…
- [ ] **API å¯†é’¥**: è‡³å°‘é…ç½®ä¸€ä¸ª LLM å’Œä¸€ä¸ªæœç´¢å¼•æ“Žçš„ API å¯†é’¥
- [ ] **æœç´¢å¼•æ“Ž**: 
  - [ ] Tavily æœç´¢æ­£å¸¸
  - [ ] DuckDuckGo æœç´¢æ­£å¸¸
  - [ ] ArXiv æœç´¢æ­£å¸¸
- [ ] **Browser-Use å·¥å…·**:
  - [ ] å·¥å…·åˆå§‹åŒ–æˆåŠŸ
  - [ ] æœç´¢å’Œæå–åŠŸèƒ½æ­£å¸¸
  - [ ] è¡¨å•å¡«å†™åŠŸèƒ½æ­£å¸¸
  - [ ] è‡ªå®šä¹‰ä»»åŠ¡æ‰§è¡Œæ­£å¸¸
- [ ] **LangGraph Studio**:
  - [ ] é…ç½®æ–‡ä»¶æ­£ç¡®åˆ›å»º
  - [ ] LangSmith è¿žæŽ¥æ­£å¸¸
  - [ ] å·¥ä½œæµå¯è§†åŒ–æ˜¾ç¤º
  - [ ] äº¤äº’å¼è°ƒè¯•åŠŸèƒ½
- [ ] **å…¶ä»–å·¥å…·**:
  - [ ] ä»£ç æ‰§è¡Œå·¥å…·æ­£å¸¸
  - [ ] æ–‡ä»¶å¤„ç†å·¥å…·æ­£å¸¸
- [ ] **å®Œæ•´æµç¨‹**:
  - [ ] ç ”ç©¶æçº²ç”Ÿæˆæ­£å¸¸
  - [ ] å†…å®¹ç”ŸæˆåŒ…å«å¼•ç”¨æ¥æº
  - [ ] æœ€ç»ˆæŠ¥å‘Šå¯¼å‡ºæˆåŠŸ
- [ ] **å¼•ç”¨æ¥æº**: æŠ¥å‘Šä¸­æ­£ç¡®æ˜¾ç¤ºå‚è€ƒèµ„æ–™

---

## ðŸŽ¯ æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬
cat > benchmark_tools.py << 'EOF'
#!/usr/bin/env python3
"""å·¥å…·æ€§èƒ½åŸºå‡†æµ‹è¯•"""

import asyncio
import time
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from tools.search_engines import SearchEngineManager
from workflow.graph import ResearchWorkflow

async def benchmark_search_engines():
    """æœç´¢å¼•æ“Žæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("â±ï¸ æœç´¢å¼•æ“Žæ€§èƒ½æµ‹è¯•...")
    
    manager = SearchEngineManager()
    queries = ["äººå·¥æ™ºèƒ½", "åŒºå—é“¾æŠ€æœ¯", "é‡å­è®¡ç®—"]
    
    for engine in manager.get_available_engines():
        print(f"\nðŸ” æµ‹è¯• {engine}:")
        total_time = 0
        total_results = 0
        
        for query in queries:
            start_time = time.time()
            results = manager.search(query, engine=engine, max_results=5)
            end_time = time.time()
            
            query_time = end_time - start_time
            total_time += query_time
            total_results += len(results)
            
            print(f"  æŸ¥è¯¢ '{query}': {len(results)} ç»“æžœ, {query_time:.2f}s")
        
        avg_time = total_time / len(queries)
        avg_results = total_results / len(queries)
        print(f"  å¹³å‡: {avg_results:.1f} ç»“æžœ/æŸ¥è¯¢, {avg_time:.2f}s/æŸ¥è¯¢")

async def benchmark_research_workflow():
    """ç ”ç©¶æµç¨‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nâ±ï¸ ç ”ç©¶æµç¨‹æ€§èƒ½æµ‹è¯•...")
    
    workflow = ResearchWorkflow(
        llm_provider="deepseek",
        max_sections=2,
        interactive_mode=False
    )
    
    test_topics = ["æœºå™¨å­¦ä¹ åŸºç¡€", "ç½‘ç»œå®‰å…¨æ¦‚è¿°"]
    
    for topic in test_topics:
        print(f"\nðŸ“ æµ‹è¯•ä¸»é¢˜: {topic}")
        
        start_time = time.time()
        outline, content_map = await workflow.run_full_workflow(topic)
        end_time = time.time()
        
        total_time = end_time - start_time
        
        if outline and content_map:
            print(f"  âœ… å®Œæˆæ—¶é—´: {total_time:.2f}s")
            print(f"  ðŸ“Š ç”Ÿæˆç« èŠ‚: {len(outline.sections)}")
            print(f"  ðŸ“ å†…å®¹éƒ¨åˆ†: {len(content_map)}")
            
            # ç»Ÿè®¡å¼•ç”¨æ¥æº
            sources_count = sum(len(content.sources) for content in content_map.values())
            print(f"  ðŸ“š å¼•ç”¨æ¥æº: {sources_count} ä¸ª")
        else:
            print(f"  âŒ å¤±è´¥ï¼Œè€—æ—¶: {total_time:.2f}s")

if __name__ == "__main__":
    asyncio.run(benchmark_search_engines())
    asyncio.run(benchmark_research_workflow())
EOF

# è¿è¡Œæ€§èƒ½æµ‹è¯•
python benchmark_tools.py
```

---

## ðŸ“ž èŽ·å–å¸®åŠ©

å¦‚æžœé‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. **æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶**: `deepresearch.log`
2. **è¿è¡Œè¯Šæ–­è„šæœ¬**: `python diagnose_tools.py`
3. **æ£€æŸ¥é…ç½®æ–‡ä»¶**: `config.yml` å’Œ `.env`
4. **éªŒè¯ API å¯†é’¥**: `python main.py config-check`
5. **æŸ¥çœ‹è¯¦ç»†é”™è¯¯**: ä½¿ç”¨ `--debug` å‚æ•°
6. **Studio ç›¸å…³é—®é¢˜**: æŸ¥çœ‹ `docs/langgraph-studio-customization.md`

---

## ðŸŽ‰ æ­å–œï¼

å¦‚æžœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œæ‚¨çš„ DeepResearch ç³»ç»Ÿå·²ç»å®Œå…¨é…ç½®å¥½ï¼ŒåŒ…æ‹¬ï¼š

âœ… **æ ¸å¿ƒåŠŸèƒ½**: æœç´¢ã€LLMã€å·¥å…·é›†æˆ  
âœ… **é«˜çº§åŠŸèƒ½**: Browser-Use æ™ºèƒ½æµè§ˆå™¨è‡ªåŠ¨åŒ–  
âœ… **å¯è§†åŒ–è°ƒè¯•**: LangGraph Studio é›†æˆ  
âœ… **å¼•ç”¨æ¥æº**: æ­£ç¡®æ˜¾ç¤ºå®žé™…åŸŸå  

çŽ°åœ¨æ‚¨å¯ä»¥ï¼š
- è¿›è¡Œé«˜è´¨é‡çš„è‡ªåŠ¨åŒ–ç ”ç©¶
- åœ¨ LangGraph Studio ä¸­å¯è§†åŒ–è°ƒè¯•å·¥ä½œæµ
- åˆ›å»ºè‡ªå®šä¹‰ç ”ç©¶æ¨¡æ¿å’Œå·¥ä½œæµ

è®°å¾—å®šæœŸæ›´æ–°ä¾èµ–åŒ…å¹¶æ£€æŸ¥ API å¯†é’¥çš„æœ‰æ•ˆæ€§ã€‚ðŸš€ 