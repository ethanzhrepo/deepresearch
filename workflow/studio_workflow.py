"""
ä¸“ä¸º LangGraph Studio è®¾è®¡çš„ç ”ç©¶å·¥ä½œæµ
æä¾›å®Œæ•´çš„å¯è§†åŒ–è°ƒè¯•å’ŒçŠ¶æ€ç®¡ç†åŠŸèƒ½
"""

import asyncio
import json
import time
from typing import Dict, Any, List, Optional, TypedDict, Literal, Annotated
from dataclasses import dataclass

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.message import add_messages

from config import config
from utils.logger import LoggerMixin
from utils.json_utils import ResearchOutline, extract_outline_from_text
from utils.markdown_export import ResearchContent
from llm.base import LLMWrapper
from llm.openai import OpenAIWrapper
from llm.claude import ClaudeWrapper
from llm.gemini import GeminiWrapper
from llm.ollama import OllamaWrapper
from llm.deepseek import DeepSeekWrapper
from tools.search_engines import SearchEngineManager


# Studio ä¼˜åŒ–çš„çŠ¶æ€å®šä¹‰
class StudioResearchState(TypedDict):
    """LangGraph Studio ä¼˜åŒ–çš„ç ”ç©¶çŠ¶æ€"""
    # åŸºç¡€ä¿¡æ¯
    topic: str
    research_depth: Literal["basic", "intermediate", "advanced"]
    language: str
    llm_provider: str
    
    # å¤§çº²ç›¸å…³
    outline: Optional[Dict[str, Any]]
    outline_approved: bool
    outline_feedback: Optional[str]
    
    # å†…å®¹ç”Ÿæˆ
    current_section: int
    current_subsection: int
    content_map: Dict[str, Any]
    
    # æœç´¢å’Œå·¥å…·
    search_results: List[Dict[str, Any]]
    search_engines_used: List[str]
    
    # æµç¨‹æ§åˆ¶
    stage: Literal["init", "outline", "outline_review", "search", "content", "review", "complete", "error"]
    error_message: Optional[str]
    debug_info: Dict[str, Any]
    
    # Studio ç‰¹æœ‰åŠŸèƒ½
    user_interventions: List[Dict[str, Any]]
    performance_metrics: Dict[str, Any]
    execution_log: List[str]
    
    # æ¶ˆæ¯å†å²ï¼ˆStudio éœ€è¦ï¼‰
    messages: Annotated[List[Dict[str, Any]], add_messages]


class StudioResearchWorkflow(LoggerMixin):
    """ä¸“ä¸º LangGraph Studio è®¾è®¡çš„ç ”ç©¶å·¥ä½œæµ"""
    
    def __init__(self):
        """åˆå§‹åŒ– Studio å·¥ä½œæµ"""
        self.memory = MemorySaver()
        self.search_manager = SearchEngineManager()
        self.graph = self._build_studio_graph()
    
    def _build_studio_graph(self) -> StateGraph:
        """æ„å»ºé€‚ç”¨äº LangGraph Studio çš„ç ”ç©¶å›¾"""
        
        workflow = StateGraph(StudioResearchState)
        
        # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
        workflow.add_node("initialize", self._initialize_node)
        workflow.add_node("generate_outline", self._generate_outline_node)
        workflow.add_node("review_outline", self._review_outline_node)
        workflow.add_node("search_information", self._search_information_node)
        workflow.add_node("generate_content", self._generate_content_node)
        workflow.add_node("review_content", self._review_content_node)
        workflow.add_node("finalize_report", self._finalize_report_node)
        workflow.add_node("handle_error", self._handle_error_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("initialize")
        
        # æ·»åŠ è¾¹å’Œæ¡ä»¶è¾¹
        workflow.add_edge("initialize", "generate_outline")
        
        # å¤§çº²ç”Ÿæˆåçš„æ¡ä»¶åˆ†æ”¯
        workflow.add_conditional_edges(
            "generate_outline",
            self._should_review_outline,
            {
                "review": "review_outline",
                "approve": "search_information",
                "error": "handle_error"
            }
        )
        
        # å¤§çº²å®¡æ ¸åçš„æ¡ä»¶åˆ†æ”¯
        workflow.add_conditional_edges(
            "review_outline", 
            self._outline_review_decision,
            {
                "regenerate": "generate_outline",
                "approve": "search_information",
                "error": "handle_error"
            }
        )
        
        # å†…å®¹ç”Ÿæˆæµç¨‹
        workflow.add_edge("search_information", "generate_content")
        workflow.add_edge("generate_content", "review_content")
        
        # å†…å®¹å®¡æ ¸åçš„æ¡ä»¶åˆ†æ”¯
        workflow.add_conditional_edges(
            "review_content",
            self._content_review_decision,
            {
                "revise": "generate_content", 
                "finalize": "finalize_report",
                "error": "handle_error"
            }
        )
        
        # ç»“æŸèŠ‚ç‚¹
        workflow.add_edge("finalize_report", END)
        workflow.add_edge("handle_error", END)
        
        return workflow.compile(checkpointer=self.memory)
    
    def _initialize_llm(self, provider: str) -> LLMWrapper:
        """åˆå§‹åŒ– LLM"""
        llm_config = config.get_llm_config(provider)
        
        if provider == "openai":
            return OpenAIWrapper(llm_config)
        elif provider == "claude":
            return ClaudeWrapper(llm_config)
        elif provider == "gemini":
            return GeminiWrapper(llm_config)
        elif provider == "ollama":
            return OllamaWrapper(llm_config)
        elif provider == "deepseek":
            return DeepSeekWrapper(llm_config)
        else:
            self.log_warning(f"Unknown provider {provider}, falling back to DeepSeek")
            return DeepSeekWrapper(config.get_llm_config("deepseek"))
    
    async def _initialize_node(self, state: StudioResearchState) -> StudioResearchState:
        """åˆå§‹åŒ–ç ”ç©¶æµç¨‹ - Studio å¯è§†åŒ–"""
        start_time = time.time()
        
        # è®°å½•åˆå§‹åŒ–æ—¥å¿—
        log_entry = f"[{time.strftime('%H:%M:%S')}] ğŸš€ åˆå§‹åŒ–ç ”ç©¶: {state.get('topic', 'Unknown')}"
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Init] {log_entry}")
        
        # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡
        performance_metrics = {
            "start_time": start_time,
            "node_execution_times": {},
            "total_api_calls": 0,
            "total_search_queries": 0
        }
        
        # åˆå§‹åŒ–è°ƒè¯•ä¿¡æ¯
        debug_info = {
            "initialized": True,
            "timestamp": start_time,
            "config": {
                "research_depth": state.get("research_depth", "intermediate"),
                "language": state.get("language", "zh-CN"),
                "llm_provider": state.get("llm_provider", "deepseek")
            },
            "available_search_engines": self.search_manager.get_available_engines()
        }
        
        return {
            **state,
            "stage": "init",
            "current_section": 0,
            "current_subsection": 0,
            "content_map": {},
            "search_results": [],
            "search_engines_used": [],
            "user_interventions": [],
            "performance_metrics": performance_metrics,
            "execution_log": execution_log,
            "debug_info": debug_info,
            "messages": state.get("messages", [])
        }
    
    async def _generate_outline_node(self, state: StudioResearchState) -> StudioResearchState:
        """ç”Ÿæˆç ”ç©¶å¤§çº² - Studio å¯è§†åŒ–"""
        start_time = time.time()
        
        # è®°å½•æ—¥å¿—
        log_entry = f"[{time.strftime('%H:%M:%S')}] ğŸ“‹ ç”Ÿæˆç ”ç©¶å¤§çº²..."
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Outline] {log_entry}")
        
        try:
            # åˆå§‹åŒ– LLM
            llm = self._initialize_llm(state.get("llm_provider", "deepseek"))
            
            # æ„å»ºå¤§çº²ç”Ÿæˆæç¤º
            depth = state.get("research_depth", "intermediate")
            section_count = {"basic": 3, "intermediate": 5, "advanced": 8}[depth]
            
            system_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ã€‚è¯·ä¸ºç»™å®šçš„ç ”ç©¶ä¸»é¢˜ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ç ”ç©¶æçº²ã€‚
            
            ç ”ç©¶æ·±åº¦: {depth}
            è¦æ±‚ç« èŠ‚æ•°: {section_count}
            è¯­è¨€: {state.get('language', 'zh-CN')}
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›æçº²ï¼ŒåŒ…å«æ ‡é¢˜ã€æ‘˜è¦ã€ç« èŠ‚åˆ—è¡¨ç­‰ã€‚
            """
            
            user_prompt = f"è¯·ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜ç”Ÿæˆè¯¦ç»†çš„ç ”ç©¶æçº²ï¼š{state['topic']}"
            
            # è°ƒç”¨ LLM
            response = llm.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.7
            )
            
            execution_time = time.time() - start_time
            
            if response.is_success:
                # è§£æå¤§çº²
                outline = extract_outline_from_text(response.content)
                
                if outline:
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥æ”¯æŒ Studio
                    outline_dict = {
                        "title": outline.title,
                        "abstract": outline.abstract,
                        "sections": [
                            {
                                "title": section.title,
                                "description": section.description,
                                "keywords": section.keywords,
                                "estimated_length": section.estimated_length,
                                "subsections": [
                                    {
                                        "title": sub.title,
                                        "description": sub.description,
                                        "keywords": sub.keywords,
                                        "estimated_length": sub.estimated_length
                                    }
                                    for sub in section.subsections
                                ]
                            }
                            for section in outline.sections
                        ],
                        "keywords": outline.keywords,
                        "estimated_total_length": outline.estimated_total_length,
                        "language": outline.language
                    }
                    
                    # æ›´æ–°çŠ¶æ€
                    state["outline"] = outline_dict
                    state["stage"] = "outline"
                    state["outline_approved"] = False
                    
                    # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                    metrics = state.get("performance_metrics", {})
                    metrics["node_execution_times"]["generate_outline"] = execution_time
                    metrics["total_api_calls"] = metrics.get("total_api_calls", 0) + 1
                    state["performance_metrics"] = metrics
                    
                    # æ›´æ–°è°ƒè¯•ä¿¡æ¯
                    debug_info = state.get("debug_info", {})
                    debug_info["outline_generated"] = True
                    debug_info["outline_sections_count"] = len(outline.sections)
                    debug_info["last_llm_call"] = {
                        "model": state.get("llm_provider"),
                        "tokens": response.token_usage,
                        "execution_time": execution_time
                    }
                    state["debug_info"] = debug_info
                    
                    execution_log.append(f"[{time.strftime('%H:%M:%S')}] âœ… å¤§çº²ç”ŸæˆæˆåŠŸ ({len(outline.sections)} ä¸ªç« èŠ‚)")
                else:
                    raise ValueError("Failed to parse outline from response")
            else:
                raise Exception(f"LLM request failed: {response.error}")
                
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            state["error_message"] = error_msg
            state["stage"] = "error"
            
            execution_log.append(f"[{time.strftime('%H:%M:%S')}] âŒ å¤§çº²ç”Ÿæˆå¤±è´¥: {error_msg}")
            
            self.log_error(f"Outline generation failed: {e}")
        
        state["execution_log"] = execution_log
        return state
    
    def _should_review_outline(self, state: StudioResearchState) -> str:
        """å†³å®šæ˜¯å¦éœ€è¦å®¡æ ¸å¤§çº²"""
        if state.get("error_message"):
            return "error"
        elif state.get("outline"):
            # åœ¨ Studio ä¸­ï¼Œå¯ä»¥è®¾ç½®è‡ªåŠ¨å®¡æ ¸æˆ–äººå·¥å®¡æ ¸
            research_depth = state.get("research_depth", "intermediate")
            if research_depth == "advanced":
                return "review"  # é«˜çº§ç ”ç©¶éœ€è¦å®¡æ ¸
            else:
                return "approve"  # åŸºç¡€å’Œä¸­çº§ç ”ç©¶è‡ªåŠ¨æ‰¹å‡†
        else:
            return "error"
    
    async def _review_outline_node(self, state: StudioResearchState) -> StudioResearchState:
        """å®¡æ ¸ç ”ç©¶å¤§çº² - Studio äº¤äº’ç‚¹"""
        log_entry = f"[{time.strftime('%H:%M:%S')}] ğŸ” å®¡æ ¸ç ”ç©¶å¤§çº²..."
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Review] {log_entry}")
        
        # åœ¨ Studio ä¸­ï¼Œè¿™é‡Œå¯ä»¥æš‚åœç­‰å¾…ç”¨æˆ·è¾“å…¥
        # æš‚æ—¶è®¾ä¸ºè‡ªåŠ¨æ‰¹å‡†ï¼Œç”¨æˆ·å¯ä»¥åœ¨ Studio ä¸­æ‰‹åŠ¨å¹²é¢„
        
        # æ¨¡æ‹Ÿå®¡æ ¸è¿‡ç¨‹
        await asyncio.sleep(1)
        
        # è®°å½•ç”¨æˆ·å¹²é¢„
        intervention = {
            "timestamp": time.time(),
            "node": "review_outline",
            "action": "auto_approve",
            "reason": "Studio auto-review mode"
        }
        
        interventions = state.get("user_interventions", [])
        interventions.append(intervention)
        
        state["user_interventions"] = interventions
        state["outline_approved"] = True
        state["stage"] = "outline_review"
        
        execution_log.append(f"[{time.strftime('%H:%M:%S')}] âœ… å¤§çº²å®¡æ ¸é€šè¿‡")
        state["execution_log"] = execution_log
        
        return state
    
    def _outline_review_decision(self, state: StudioResearchState) -> str:
        """å¤§çº²å®¡æ ¸å†³ç­–"""
        if state.get("error_message"):
            return "error"
        elif state.get("outline_approved"):
            return "approve"
        else:
            return "regenerate"
    
    async def _search_information_node(self, state: StudioResearchState) -> StudioResearchState:
        """æœç´¢ä¿¡æ¯ - Studio å¯è§†åŒ–æœç´¢è¿‡ç¨‹"""
        start_time = time.time()
        
        log_entry = f"[{time.strftime('%H:%M:%S')}] ğŸ” æœç´¢ç›¸å…³ä¿¡æ¯..."
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Search] {log_entry}")
        
        try:
            outline = state.get("outline", {})
            topic = state.get("topic", "")
            
            # ä»å¤§çº²ä¸­æå–æœç´¢å…³é”®è¯
            search_keywords = []
            if outline:
                search_keywords.extend(outline.get("keywords", []))
                for section in outline.get("sections", []):
                    search_keywords.extend(section.get("keywords", []))
            
            # å¦‚æœæ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨ä¸»é¢˜
            if not search_keywords:
                search_keywords = [topic]
            
            # ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“
            available_engines = self.search_manager.get_available_engines()
            search_engines_used = []
            all_search_results = []
            
            # é™åˆ¶æœç´¢å¼•æ“æ•°é‡ä»¥é¿å…è¿‡é•¿ç­‰å¾…
            engines_to_use = available_engines[:3]  # ä½¿ç”¨å‰3ä¸ªå¯ç”¨å¼•æ“
            
            for engine in engines_to_use:
                try:
                    # æœç´¢å‰å‡ ä¸ªå…³é”®è¯
                    for keyword in search_keywords[:2]:  # é™åˆ¶å…³é”®è¯æ•°é‡
                        results = self.search_manager.search(
                            query=keyword,
                            engine=engine,
                            max_results=3
                        )
                        
                        if results:
                            search_engines_used.append(engine)
                            for result in results:
                                all_search_results.append({
                                    "engine": engine,
                                    "keyword": keyword,
                                    "title": result.title,
                                    "url": result.url,
                                    "snippet": result.snippet,
                                    "source": result.source
                                })
                            
                        await asyncio.sleep(0.5)  # é¿å…è¿‡å¿«è¯·æ±‚
                        
                except Exception as e:
                    self.log_warning(f"Search failed for {engine}: {e}")
                    continue
            
            execution_time = time.time() - start_time
            
            # æ›´æ–°çŠ¶æ€
            state["search_results"] = all_search_results
            state["search_engines_used"] = list(set(search_engines_used))
            state["stage"] = "search"
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            metrics = state.get("performance_metrics", {})
            metrics["node_execution_times"]["search_information"] = execution_time
            metrics["total_search_queries"] = metrics.get("total_search_queries", 0) + len(search_keywords) * len(engines_to_use)
            state["performance_metrics"] = metrics
            
            # æ›´æ–°è°ƒè¯•ä¿¡æ¯
            debug_info = state.get("debug_info", {})
            debug_info["search_completed"] = True
            debug_info["search_stats"] = {
                "total_results": len(all_search_results),
                "engines_used": search_engines_used,
                "keywords_searched": search_keywords[:2]
            }
            state["debug_info"] = debug_info
            
            execution_log.append(f"[{time.strftime('%H:%M:%S')}] âœ… æœç´¢å®Œæˆ ({len(all_search_results)} ä¸ªç»“æœ)")
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            state["error_message"] = error_msg
            state["stage"] = "error"
            
            execution_log.append(f"[{time.strftime('%H:%M:%S')}] âŒ æœç´¢å¤±è´¥: {error_msg}")
            
            self.log_error(f"Search failed: {e}")
        
        state["execution_log"] = execution_log
        return state
    
    async def _generate_content_node(self, state: StudioResearchState) -> StudioResearchState:
        """ç”Ÿæˆå†…å®¹ - Studio å¯è§†åŒ–å†…å®¹ç”Ÿæˆ"""
        start_time = time.time()
        
        log_entry = f"[{time.strftime('%H:%M:%S')}] âœï¸ ç”Ÿæˆç ”ç©¶å†…å®¹..."
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Content] {log_entry}")
        
        try:
            # åˆå§‹åŒ– LLM
            llm = self._initialize_llm(state.get("llm_provider", "deepseek"))
            
            outline = state.get("outline", {})
            search_results = state.get("search_results", [])
            
            # ç”Ÿæˆå†…å®¹æ˜ å°„
            content_map = {}
            
            # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆå†…å®¹
            for i, section in enumerate(outline.get("sections", [])):
                section_key = f"section_{i+1}"
                
                # æ„å»ºå†…å®¹ç”Ÿæˆæç¤º
                search_context = ""
                if search_results:
                    # é€‰æ‹©ç›¸å…³çš„æœç´¢ç»“æœ
                    relevant_results = [r for r in search_results if any(
                        keyword.lower() in r.get("title", "").lower() or 
                        keyword.lower() in r.get("snippet", "").lower()
                        for keyword in section.get("keywords", [])
                    )][:5]  # æœ€å¤š5ä¸ªç›¸å…³ç»“æœ
                    
                    if relevant_results:
                        search_context = "\n\nå‚è€ƒèµ„æ–™:\n"
                        for result in relevant_results:
                            search_context += f"- {result.get('title', '')} ({result.get('source', '')})\n"
                            search_context += f"  {result.get('snippet', '')[:200]}...\n"
                
                content_prompt = f"""
                è¯·ä¸ºä»¥ä¸‹ç« èŠ‚ç”Ÿæˆè¯¦ç»†å†…å®¹ï¼š
                
                ç« èŠ‚æ ‡é¢˜: {section.get('title', '')}
                ç« èŠ‚æè¿°: {section.get('description', '')}
                å…³é”®è¯: {', '.join(section.get('keywords', []))}
                ç›®æ ‡é•¿åº¦: {section.get('estimated_length', 800)} å­—
                
                {search_context}
                
                è¦æ±‚:
                1. å†…å®¹åº”è¯¥ä¸“ä¸šã€å‡†ç¡®ã€æœ‰æ·±åº¦
                2. ä½¿ç”¨ {state.get('language', 'zh-CN')} è¯­è¨€
                3. åŒ…å«é€‚å½“çš„å¼•ç”¨å’Œå‚è€ƒ
                4. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘è¿è´¯
                """
                
                response = llm.generate(
                    prompt=content_prompt,
                    max_tokens=2000,
                    temperature=0.7
                )
                
                if response.is_success:
                    # æå–å¼•ç”¨æ¥æº
                    sources = []
                    if search_results:
                        relevant_results = [r for r in search_results if any(
                            keyword.lower() in r.get("title", "").lower() or 
                            keyword.lower() in r.get("snippet", "").lower()
                            for keyword in section.get("keywords", [])
                        )][:3]  # æœ€å¤š3ä¸ªæ¥æº
                        
                        sources = [f"{r.get('title', '')} - {r.get('source', '')}" for r in relevant_results]
                    
                    content_map[section_key] = {
                        "title": section.get("title", ""),
                        "content": response.content,
                        "sources": sources,
                        "keywords": section.get("keywords", [])
                    }
                    
                    execution_log.append(f"[{time.strftime('%H:%M:%S')}] âœ… ç« èŠ‚ {i+1} å†…å®¹ç”Ÿæˆå®Œæˆ")
                else:
                    self.log_warning(f"Failed to generate content for section {i+1}")
                
                # æ·»åŠ è¿›åº¦å»¶è¿Ÿï¼Œè®© Studio å¯ä»¥çœ‹åˆ°è¿›åº¦
                await asyncio.sleep(0.5)
            
            execution_time = time.time() - start_time
            
            # æ›´æ–°çŠ¶æ€
            state["content_map"] = content_map
            state["stage"] = "content"
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            metrics = state.get("performance_metrics", {})
            metrics["node_execution_times"]["generate_content"] = execution_time
            metrics["total_api_calls"] = metrics.get("total_api_calls", 0) + len(outline.get("sections", []))
            state["performance_metrics"] = metrics
            
            # æ›´æ–°è°ƒè¯•ä¿¡æ¯
            debug_info = state.get("debug_info", {})
            debug_info["content_generated"] = True
            debug_info["content_stats"] = {
                "sections_generated": len(content_map),
                "total_content_length": sum(len(c.get("content", "")) for c in content_map.values())
            }
            state["debug_info"] = debug_info
            
            execution_log.append(f"[{time.strftime('%H:%M:%S')}] âœ… æ‰€æœ‰å†…å®¹ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            state["error_message"] = error_msg
            state["stage"] = "error"
            
            execution_log.append(f"[{time.strftime('%H:%M:%S')}] âŒ å†…å®¹ç”Ÿæˆå¤±è´¥: {error_msg}")
            
            self.log_error(f"Content generation failed: {e}")
        
        state["execution_log"] = execution_log
        return state
    
    async def _review_content_node(self, state: StudioResearchState) -> StudioResearchState:
        """å®¡æ ¸å†…å®¹ - Studio å¯è§†åŒ–å®¡æ ¸"""
        log_entry = f"[{time.strftime('%H:%M:%S')}] ğŸ” å®¡æ ¸ç”Ÿæˆå†…å®¹..."
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Content Review] {log_entry}")
        
        # ç®€å•çš„å†…å®¹è´¨é‡æ£€æŸ¥
        content_map = state.get("content_map", {})
        
        quality_score = 0
        total_sections = len(content_map)
        
        if total_sections > 0:
            for section_key, content in content_map.items():
                content_text = content.get("content", "")
                
                # æ£€æŸ¥å†…å®¹é•¿åº¦
                if len(content_text) > 200:
                    quality_score += 1
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨æ¥æº
                if content.get("sources"):
                    quality_score += 1
            
            quality_ratio = quality_score / (total_sections * 2)  # æ¯ä¸ªç« èŠ‚æœ€å¤š2åˆ†
        else:
            quality_ratio = 0
        
        # è®°å½•å®¡æ ¸ç»“æœ
        intervention = {
            "timestamp": time.time(),
            "node": "review_content",
            "action": "quality_check",
            "quality_score": quality_ratio,
            "details": f"Generated {total_sections} sections with {quality_score}/{total_sections*2} quality points"
        }
        
        interventions = state.get("user_interventions", [])
        interventions.append(intervention)
        
        state["user_interventions"] = interventions
        state["stage"] = "review"
        
        execution_log.append(f"[{time.strftime('%H:%M:%S')}] âœ… å†…å®¹å®¡æ ¸å®Œæˆ (è´¨é‡åˆ†æ•°: {quality_ratio:.2f})")
        state["execution_log"] = execution_log
        
        return state
    
    def _content_review_decision(self, state: StudioResearchState) -> str:
        """å†…å®¹å®¡æ ¸å†³ç­–"""
        if state.get("error_message"):
            return "error"
        
        # æ£€æŸ¥å†…å®¹è´¨é‡
        interventions = state.get("user_interventions", [])
        latest_review = None
        
        for intervention in reversed(interventions):
            if intervention.get("node") == "review_content":
                latest_review = intervention
                break
        
        if latest_review:
            quality_score = latest_review.get("quality_score", 0)
            if quality_score >= 0.7:  # è´¨é‡åˆ†æ•°é˜ˆå€¼
                return "finalize"
            else:
                return "revise"
        else:
            return "finalize"  # é»˜è®¤å®Œæˆ
    
    async def _finalize_report_node(self, state: StudioResearchState) -> StudioResearchState:
        """å®ŒæˆæŠ¥å‘Š - Studio å¯è§†åŒ–æœ€ç»ˆæ­¥éª¤"""
        log_entry = f"[{time.strftime('%H:%M:%S')}] ğŸ“„ å®Œæˆç ”ç©¶æŠ¥å‘Š..."
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Finalize] {log_entry}")
        
        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        start_time = state.get("performance_metrics", {}).get("start_time", time.time())
        total_time = time.time() - start_time
        
        # æ›´æ–°æœ€ç»ˆçŠ¶æ€
        state["stage"] = "complete"
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        metrics = state.get("performance_metrics", {})
        metrics["total_execution_time"] = total_time
        metrics["completion_timestamp"] = time.time()
        state["performance_metrics"] = metrics
        
        # æœ€ç»ˆè°ƒè¯•ä¿¡æ¯
        debug_info = state.get("debug_info", {})
        debug_info["workflow_completed"] = True
        debug_info["final_stats"] = {
            "total_execution_time": total_time,
            "sections_count": len(state.get("content_map", {})),
            "search_results_count": len(state.get("search_results", [])),
            "interventions_count": len(state.get("user_interventions", []))
        }
        state["debug_info"] = debug_info
        
        execution_log.append(f"[{time.strftime('%H:%M:%S')}] ğŸ‰ ç ”ç©¶æŠ¥å‘Šå®Œæˆ! (æ€»è€—æ—¶: {total_time:.2f}s)")
        state["execution_log"] = execution_log
        
        return state
    
    async def _handle_error_node(self, state: StudioResearchState) -> StudioResearchState:
        """å¤„ç†é”™è¯¯ - Studio é”™è¯¯å¯è§†åŒ–"""
        error_msg = state.get("error_message", "Unknown error")
        
        log_entry = f"[{time.strftime('%H:%M:%S')}] âŒ å¤„ç†é”™è¯¯: {error_msg}"
        execution_log = state.get("execution_log", [])
        execution_log.append(log_entry)
        
        print(f"[Studio Error] {log_entry}")
        
        # è®°å½•é”™è¯¯å¹²é¢„
        intervention = {
            "timestamp": time.time(),
            "node": "handle_error",
            "action": "error_handling",
            "error_message": error_msg
        }
        
        interventions = state.get("user_interventions", [])
        interventions.append(intervention)
        
        state["user_interventions"] = interventions
        state["stage"] = "error"
        state["execution_log"] = execution_log
        
        return state


# Studio å·¥ä½œæµçš„å·¥å‚å‡½æ•°
def create_studio_workflow() -> StudioResearchWorkflow:
    """åˆ›å»º Studio å·¥ä½œæµå®ä¾‹"""
    return StudioResearchWorkflow()


# ç›´æ¥å¯¼å‡ºå›¾å®ä¾‹ç»™ Studio ä½¿ç”¨
studio_workflow = create_studio_workflow()
graph = studio_workflow.graph

# ä¸º Studio æä¾›çš„ä¾¿æ·å‡½æ•°
async def run_studio_research(
    topic: str,
    research_depth: str = "intermediate",
    language: str = "zh-CN",
    llm_provider: str = "deepseek",
    thread_id: str = None
) -> Dict[str, Any]:
    """
    è¿è¡Œ Studio ç ”ç©¶å·¥ä½œæµ
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        research_depth: ç ”ç©¶æ·±åº¦ (basic/intermediate/advanced)
        language: è¯­è¨€
        llm_provider: LLM æä¾›å•†
        thread_id: çº¿ç¨‹ID (Studio ä¼šè¯ç®¡ç†)
    
    Returns:
        ç ”ç©¶ç»“æœçŠ¶æ€
    """
    
    # åˆå§‹çŠ¶æ€
    initial_state = {
        "topic": topic,
        "research_depth": research_depth,
        "language": language,
        "llm_provider": llm_provider,
        "outline": None,
        "outline_approved": False,
        "outline_feedback": None,
        "current_section": 0,
        "current_subsection": 0,
        "content_map": {},
        "search_results": [],
        "search_engines_used": [],
        "stage": "init",
        "error_message": None,
        "debug_info": {},
        "user_interventions": [],
        "performance_metrics": {},
        "execution_log": [],
        "messages": []
    }
    
    # é…ç½®
    config = {
        "configurable": {
            "thread_id": thread_id or f"studio-research-{int(time.time())}"
        }
    }
    
    # è¿è¡Œå·¥ä½œæµ
    try:
        result = await graph.ainvoke(initial_state, config=config)
        return result
    except Exception as e:
        print(f"Studio workflow error: {e}")
        return {
            **initial_state,
            "stage": "error",
            "error_message": str(e)
        }


if __name__ == "__main__":
    """Studio æœ¬åœ°æµ‹è¯•"""
    import asyncio
    
    async def test_studio_workflow():
        result = await run_studio_research(
            topic="äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
            research_depth="intermediate",
            language="zh-CN",
            llm_provider="deepseek"
        )
        
        print("\nğŸ¨ Studio å·¥ä½œæµæµ‹è¯•ç»“æœ:")
        print(f"é˜¶æ®µ: {result.get('stage')}")
        print(f"ä¸»é¢˜: {result.get('topic')}")
        
        if result.get('outline'):
            print(f"å¤§çº²: {result['outline'].get('title')}")
        
        if result.get('content_map'):
            print(f"å†…å®¹ç« èŠ‚: {len(result['content_map'])}")
        
        if result.get('execution_log'):
            print("\næ‰§è¡Œæ—¥å¿—:")
            for log in result['execution_log'][-5:]:  # æ˜¾ç¤ºæœ€å5æ¡æ—¥å¿—
                print(f"  {log}")
    
    asyncio.run(test_studio_workflow()) 